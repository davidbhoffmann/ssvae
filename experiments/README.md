# SSVAE Label Robustness and Sparsity Analysis

This folder contains experiments to evaluate the robustness of Semi-Supervised Variational Autoencoders (SSVAE) to label quality and quantity.

## ðŸš€ New: Multi-GPU Support & Performance Optimizations

**Major performance improvements added!** Experiments now run **4-8x faster** with multi-GPU support and optimized data loading.

### Quick Performance Gains

- **Multi-GPU Training**: Automatic support for 2-8 GPUs (3-4x speedup with 4 GPUs)
- **Faster Data Loading**: Parallel workers (1.5-2x speedup)
- **Reduced Evaluation**: Configurable eval frequency (1.2-1.3x speedup)
- **Combined**: Up to 8x faster training!

### Quick Start with Optimizations

```bash
# Automatic multi-GPU (uses all available GPUs)
python combined_robustness_experiment.py --dataset MNIST --num_epochs 75

# Specify GPUs and workers
python combined_robustness_experiment.py \
    --dataset MNIST \
    --num_epochs 75 \
    --gpu_ids 0,1,2,3 \
    --num_workers 8 \
    --eval_frequency 5

# SLURM submission (multi-GPU)
sbatch submit_multi_gpu.sh
```

**ðŸ“– See [MULTI_GPU_GUIDE.md](MULTI_GPU_GUIDE.md) and [PERFORMANCE_GUIDE.md](PERFORMANCE_GUIDE.md) for detailed documentation.**

## Overview

The experiments test the "break point" of the SSVAE framework by systematically varying:

1. **Label Quantity**: Reduce the percentage of labeled data (from 10% down to 0.1%)
2. **Label Noise**: Introduce corrupted labels (flip 10-30% of labels to wrong classes)

The goal is to measure how sensitive disentanglement quality is to supervision quality, specifically:
- Does the latent space collapse when 20% of labels are wrong?
- How much labeled data is needed to maintain good disentanglement?
- What is the relationship between classification accuracy and disentanglement metrics?

## Files

### Core Files
- `ssvae_model.py`: SSVAE encoder/decoder architecture using probtorch (now with multi-GPU support)
- `utils.py`: Utilities for data loading, label corruption, and training (optimized)
- `metrics.py`: Implementation of disentanglement metrics (Beta-VAE, Factor-VAE, MIG)
- `label_robustness_experiment.py`: Label robustness experiment (multi-GPU enabled)
- `combined_robustness_experiment.py`: Combined 3D parameter sweep (multi-GPU enabled)

### New Files
- `submit_multi_gpu.sh`: SLURM script for multi-GPU jobs
- `test_performance.sh`: Performance testing and benchmarking script
- `MULTI_GPU_GUIDE.md`: Comprehensive multi-GPU usage guide
- `PERFORMANCE_GUIDE.md`: Performance optimization and benchmarking guide
- `README.md`: This file

## Installation

Ensure you have the required dependencies:

```bash
pip install torch torchvision probtorch numpy scipy scikit-learn matplotlib seaborn
```

Or use the requirements file:

```bash
pip install -r requirements.txt
```

## Usage

### Quick Start

Run experiments on both MNIST and FashionMNIST with optimizations:

```bash
# Basic usage (automatic multi-GPU)
python label_robustness_experiment.py

# With performance optimizations
python label_robustness_experiment.py \
    --num_workers 8 \
    --eval_frequency 5
```

### Performance Testing

Test your setup and measure speedup:

```bash
# Run automated performance tests
./test_performance.sh
```

This will:
- Test baseline performance
- Test with optimized data loading
- Test multi-GPU (if available)
- Show recommended settings for your system

### Multi-GPU Options

```bash
# Use specific GPUs
python label_robustness_experiment.py --gpu_ids 0,1,2,3

# Disable multi-GPU
python label_robustness_experiment.py --no_multi_gpu

# Optimize for 4 GPUs
python label_robustness_experiment.py \
    --gpu_ids 0,1,2,3 \
    --num_workers 16 \
    --eval_frequency 5
```

### Run on Specific Dataset

```bash
# MNIST only
python label_robustness_experiment.py --dataset MNIST

# FashionMNIST only
python label_robustness_experiment.py --dataset FashionMNIST
```

### Customize Training

```bash
# Shorter training for quick testing
python label_robustness_experiment.py --num_epochs 50 --dataset MNIST

# Change latent dimensions
python label_robustness_experiment.py --num_style 32

# Force CPU (no CUDA)
python label_robustness_experiment.py --cuda false
```

### Run Specific Experiments

```bash
# Only label sparsity experiment
python label_robustness_experiment.py --sparsity_only

# Only label noise experiment
python label_robustness_experiment.py --noise_only --label_fraction 0.05
```

### SLURM Cluster Usage

```bash
# Submit multi-GPU job
sbatch submit_multi_gpu.sh

# Check job status
squeue -u $USER

# View output
tail -f logs/ssvae_multi_gpu_*.out
```

## Experiments

### 1. Label Sparsity Analysis

Trains models with varying amounts of labeled data (10%, 5%, 2%, 1%, 0.5%, 0.1%) with no label corruption.

**Metrics Measured:**
- Classification accuracy
- Beta-VAE disentanglement score
- Factor-VAE score
- Mutual Information Gap (MIG)

**Expected Findings:**
- Classification accuracy should decrease as labeled data decreases
- Disentanglement quality may remain stable until a critical threshold
- Identification of the minimum labeled data needed for good representations

### 2. Label Noise Analysis

Trains models with fixed labeled data (default 10%) but varying corruption rates (0%, 5%, 10%, 15%, 20%, 25%, 30%).

**Metrics Measured:**
- Classification accuracy
- Beta-VAE disentanglement score
- Factor-VAE score
- Mutual Information Gap (MIG)

**Expected Findings:**
- Both accuracy and disentanglement should degrade with more noise
- Potential non-linear relationship between corruption and performance
- Identification of corruption tolerance threshold

## Results

Results are saved to `../results/` directory:

- `{dataset}_sparsity_results.json`: Label sparsity experiment results
- `{dataset}_noise_results.json`: Label noise experiment results
- `{dataset}_robustness_analysis.png`: Visualization plots

### Result Format

Each JSON file contains a list of experiment runs with:

```json
{
  "dataset": "MNIST",
  "label_fraction": 0.1,
  "corruption_rate": 0.0,
  "final_test_accuracy": 0.952,
  "final_test_elbo": 1234.5,
  "disentanglement_metrics": {
    "beta_vae": 0.823,
    "factor_vae": 2.145,
    "mig": 0.412
  },
  "train_elbos": [...],
  "test_elbos": [...],
  "test_accuracies": [...]
}
```

## Disentanglement Metrics

### Beta-VAE Score

Measures how well individual latent dimensions correspond to class labels. A classifier is trained to predict class labels using single latent dimensions. Higher scores (closer to 1.0) indicate better disentanglement.

### Factor-VAE Score

Measures the ratio of between-class variance to within-class variance for each latent dimension. Higher scores indicate latent dimensions that clearly separate different classes.

### Mutual Information Gap (MIG)

Measures the difference between the top two latent dimensions in terms of mutual information with class labels. Quantifies how exclusively each factor is encoded.

## Interpretation Guide

### Key Questions to Answer

1. **At what label fraction does performance collapse?**
   - Look for sharp drops in accuracy and disentanglement metrics

2. **How much noise can the model tolerate?**
   - Identify the corruption rate where metrics significantly degrade

3. **Is disentanglement more robust than accuracy?**
   - Compare relative changes in accuracy vs disentanglement metrics

4. **Does the relationship differ between datasets?**
   - Compare MNIST (simpler) vs FashionMNIST (more complex)

## Customization

### Modify Label Fractions

Edit `DEFAULT_CONFIG` in `label_robustness_experiment.py`:

```python
'label_fractions': [0.1, 0.05, 0.02, 0.01, 0.005, 0.001],
```

### Modify Corruption Rates

```python
'corruption_rates': [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3],
```

### Change Model Architecture

```python
'num_hidden': 256,  # Hidden layer size
'num_style': 50,    # Latent style dimensions
```

## Computational Requirements

### With Optimizations (New)

- **Time per experiment**: 
  - 1 GPU: ~5-10 minutes
  - 2 GPUs: ~3-5 minutes (1.7-2x speedup)
  - 4 GPUs: ~1.5-3 minutes (3-4x speedup)
- **Memory**: ~2-4GB GPU memory per GPU, ~4-8GB RAM
- **Storage**: Results are small (~100KB per experiment)

### Without Optimizations (Old)

- **Time**: Each experiment run takes ~15-30 minutes on GPU, ~60+ minutes on CPU
- **Memory**: ~2GB GPU memory, ~4GB RAM
- **Storage**: Results are small (~100KB per experiment)

### Recommended Hardware

- **Minimum**: 1 GPU (GTX 1060 or better), 8GB RAM, 4 CPU cores
- **Recommended**: 2-4 GPUs (RTX 2080 or better), 16GB RAM, 8+ CPU cores
- **Optimal**: 4+ GPUs (A100/V100), 32GB+ RAM, 16+ CPU cores

## Performance Tips

1. **Use multi-GPU**: Automatic 3-4x speedup with 4 GPUs
2. **Increase workers**: Set `--num_workers 8` for faster data loading
3. **Reduce eval frequency**: Use `--eval_frequency 5` for long runs
4. **Monitor GPUs**: Use `nvidia-smi dmon` to check utilization

See [PERFORMANCE_GUIDE.md](PERFORMANCE_GUIDE.md) for detailed optimization tips.

## Citation

If you use this code, please cite the original SSVAE paper:

```
Kingma, D. P., Rezende, D. J., Mohamed, S., & Welling, M. (2014).
Semi-supervised learning with deep generative models.
Advances in neural information processing systems, 27.
```

## Troubleshooting

### Out of Memory Errors

Reduce batch size, number of workers, or use fewer GPUs:

```bash
# Reduce workers
python label_robustness_experiment.py --num_workers 2

# Disable multi-GPU
python label_robustness_experiment.py --no_multi_gpu

# Use specific GPUs
python label_robustness_experiment.py --gpu_ids 0,1
```

Edit `DEFAULT_CONFIG['num_batch']` to reduce batch size.

### Slow Training

**First, check GPU utilization:**

```bash
nvidia-smi dmon -s u
# Target: >90% GPU utilization
```

**If GPU utilization is low (<80%):**
- Increase `--num_workers` (try 8 or 16)
- Check that data is on fast storage (SSD)

**If GPU utilization is high:**
- Use multi-GPU: `--gpu_ids 0,1,2,3`
- Reduce evaluation: `--eval_frequency 5`

**Old approach (still works):**

```bash
python label_robustness_experiment.py --num_epochs 50 --cuda true
```

### Multi-GPU Not Working

**Check GPU availability:**

```bash
nvidia-smi
python -c "import torch; print(torch.cuda.device_count())"
```

**Common issues:**
- GPUs are busy: Use `--gpu_ids` to select available GPUs
- Driver issues: Update NVIDIA drivers
- PyTorch not detecting GPUs: Reinstall PyTorch with CUDA support

### Data Loading Bottleneck

If training is slow despite high num_workers:
- Check disk I/O with `iostat -x 1`
- Consider caching data in `/dev/shm` or RAM disk
- Use faster storage (NVMe SSD)

### Import Errors

Ensure all dependencies are installed and you're running from the correct directory:

```bash
cd /path/to/ssvae/experiments
python label_robustness_experiment.py
```

### SLURM Issues

If SLURM jobs fail:
- Check logs in `logs/` directory
- Verify conda environment path in submit script
- Ensure requested resources are available
- Check job status: `squeue -u $USER`

## Additional Documentation

- **[MULTI_GPU_GUIDE.md](MULTI_GPU_GUIDE.md)**: Comprehensive guide to multi-GPU training
- **[PERFORMANCE_GUIDE.md](PERFORMANCE_GUIDE.md)**: Performance benchmarks and optimization tips
- **[GETTING_STARTED.md](GETTING_STARTED.md)**: Basic usage and setup
- **[COMBINED_EXPERIMENT.md](COMBINED_EXPERIMENT.md)**: Documentation for combined experiments
