\relax 
\AC@reset@newl@bel
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newacro{CCVAE}[CCVAE]{Characteristic Capturing VAE}
\newacro{DLVM}[DLVM]{Deep Latent Variable Model}
\newacro{ELBO}[ELBO]{evidence lower bound}
\newacro{GPPVAE}[GPPVAE]{Gaussian Process Prior VAE}
\newacro{GAN}[GAN]{Generative Adversarial Network}
\newacro{i.i.d.}[i.i.d.]{independent and identically distributed}
\newacro{M1}[M1]{latent-feature discriminative model}
\newacro{M1+M2}[M1+M2]{stacked generative semi-supervised model}
\newacro{M2}[M2]{generative semi-supervised model}
\newacro{MIG}[MIG]{Mutual Information Gap}
\newacro{MNIST}[MNIST]{Modified National Institute of Standards and Technology}
\newacro{SVHN}[SVHN]{Street View House Numbers}
\newacro{PoE}[PoE]{Product-of-Experts}
\newacro{SHOT-VAE}[SHOT-VAE]{SmootH-ELBO Optimal InTerpolation VAE}
\newacro{SSVAE}[SSVAE]{Semi-Supervised VAE}
\newacro{VAE}[VAE]{Variational Auto-Encoder}
\newacro{ReLU}[ReLU]{Rectified Linear Units}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{intro}{{1}{1}{Introduction}{section.1}{}}
\citation{kingma_auto-encoding_2013,rezende2014stochastic}
\citation{bengio2013representation}
\citation{Higgins2016betaVAELB}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Formulation}{2}{section.2}\protected@file@percent }
\newlabel{background}{{2}{2}{Background and Formulation}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Disentanglement in Variational Auto-Encoders}{2}{subsection.2.1}\protected@file@percent }
\AC@undonewlabel{acro:VAE}
\newlabel{acro:VAE}{{2.1}{2}{Disentanglement in Variational Auto-Encoders}{section*.2}{}}
\acronymused{VAE}
\acronymused{VAE}
\AC@undonewlabel{acro:ELBO}
\newlabel{acro:ELBO}{{2.1}{2}{Disentanglement in Variational Auto-Encoders}{section*.3}{}}
\acronymused{ELBO}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Degrees of Supervision}{2}{subsection.2.2}\protected@file@percent }
\citation{kingma_semi-supervised_2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Semi-Supervised Variational Auto-Encoders}{3}{subsection.2.3}\protected@file@percent }
\acronymused{VAE}
\acronymused{ELBO}
\citation{kingma_semi-supervised_2014}
\newlabel{eq:sup_obj}{{6}{4}{Semi-Supervised Variational Auto-Encoders}{equation.6}{}}
\newlabel{eq:unsup_obj}{{9}{4}{Semi-Supervised Variational Auto-Encoders}{equation.9}{}}
\newlabel{eq:combined_obj}{{10}{4}{Semi-Supervised Variational Auto-Encoders}{equation.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Importance Sampling}{4}{subsection.2.4}\protected@file@percent }
\newlabel{sec:imp_sampling}{{2.4}{4}{Importance Sampling}{subsection.2.4}{}}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {3}Semi-Supervised Disentanglement with Arbitrary Dependencies}{6}{section.3}\protected@file@percent }
\newlabel{method}{{3}{6}{Semi-Supervised Disentanglement with Arbitrary Dependencies}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Generalised Variational Objective}{6}{subsection.3.1}\protected@file@percent }
\citation{narayanaswamy_learning_2017}
\newlabel{eq:expectation_est}{{18}{7}{The Generalised Variational Objective}{equation.18}{}}
\newlabel{eq:disc_est}{{19}{7}{The Generalised Variational Objective}{equation.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Relation to Kingma's M2 Model}{7}{subsection.3.2}\protected@file@percent }
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{deng2012mnist}
\citation{netzer2011reading}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Graphical Model and Stochastic Implementation}{8}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Experiments and Findings}{8}{subsection.3.4}\protected@file@percent }
\newlabel{sec:their_eperiments}{{3.4}{8}{Experiments and Findings}{subsection.3.4}{}}
\AC@undonewlabel{acro:MNIST}
\newlabel{acro:MNIST}{{3.4}{8}{Experiments and Findings}{section*.4}{}}
\acronymused{MNIST}
\AC@undonewlabel{acro:SVHN}
\newlabel{acro:SVHN}{{3.4}{8}{Experiments and Findings}{section*.5}{}}
\acronymused{SVHN}
\acronymused{MNIST}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Benchmark Classification: MNIST and SVHN}{8}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{sec:mnist}{{3.4.1}{8}{Benchmark Classification: MNIST and SVHN}{subsubsection.3.4.1}{}}
\acronymused{MNIST}
\acronymused{SVHN}
\acronymused{MNIST}
\acronymused{SVHN}
\AC@undonewlabel{acro:SSVAE}
\newlabel{acro:SSVAE}{{3.4.1}{8}{Benchmark Classification: MNIST and SVHN}{section*.6}{}}
\acronymused{SSVAE}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{GeBeKr01}
\citation{pmlr-v38-jampani15}
\citation{pmlr-v38-jampani15}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\acronymused{MNIST}
\acronymused{SVHN}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Intrinsic Faces: Disentangling Continuous Factors}{9}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{sec:exp_faces}{{3.4.2}{9}{Intrinsic Faces: Disentangling Continuous Factors}{subsubsection.3.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Multi-MNIST: Stochastic Dimensionality and Compositionality}{9}{subsubsection.3.4.3}\protected@file@percent }
\acronymused{MNIST}
\acronymused{MNIST}
\acronymused{MNIST}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{GeBeKr01}
\citation{pmlr-v38-jampani15}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {4}Extensions and Limitations}{10}{section.4}\protected@file@percent }
\newlabel{extensions}{{4}{10}{Extensions and Limitations}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Own Criticism}{10}{subsection.4.1}\protected@file@percent }
\newlabel{sec:my_critique}{{4.1}{10}{Own Criticism}{subsection.4.1}{}}
\acronymused{SSVAE}
\newlabel{eq:yaleb}{{23}{10}{Own Criticism}{equation.23}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{pmlr-v38-jampani15}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{casale_gaussian_2018}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{locatello_challenging_2018}
\acronymused{MNIST}
\acronymused{SVHN}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}General Critiques}{11}{subsection.4.2}\protected@file@percent }
\newlabel{sec:general_critique}{{4.2}{11}{General Critiques}{subsection.4.2}{}}
\AC@undonewlabel{acro:i.i.d.}
\newlabel{acro:i.i.d.}{{4.2}{11}{General Critiques}{section*.7}{}}
\acronymused{i.i.d.}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Impossibility of Unsupervised Disentanglement}{11}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{sec:impossibility_theorem}{{4.2.1}{11}{Impossibility of Unsupervised Disentanglement}{subsubsection.4.2.1}{}}
\acronymused{VAE}
\acronymused{ELBO}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{locatello_challenging_2018}
\citation{narayanaswamy_learning_2017}
\citation{casale_gaussian_2018}
\citation{narayanaswamy_learning_2017}
\citation{casale_gaussian_2018}
\citation{casale_gaussian_2018}
\citation{casale_gaussian_2018}
\citation{narayanaswamy_learning_2017}
\citation{esmaeili_structured_2018}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}The Limitation of Isotropic Priors}{12}{subsubsection.4.2.2}\protected@file@percent }
\acronymused{i.i.d.}
\AC@undonewlabel{acro:GPPVAE}
\newlabel{acro:GPPVAE}{{4.2.2}{12}{The Limitation of Isotropic Priors}{section*.8}{}}
\acronymused{GPPVAE}
\acronymused{ELBO}
\citation{mattei_leveraging_2018}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{mattei_leveraging_2018}
\citation{narayanaswamy_learning_2017}
\citation{joy_capturing_2020}
\citation{joy_capturing_2020}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Unbounded Likelihoods and Mode Collapse}{13}{subsubsection.4.2.3}\protected@file@percent }
\AC@undonewlabel{acro:DLVM}
\newlabel{acro:DLVM}{{4.2.3}{13}{Unbounded Likelihoods and Mode Collapse}{section*.9}{}}
\acronymused{DLVM}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Disentanglement Implications and Limitations}{13}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Semantic Conflation Problem}{13}{subsubsection.4.3.1}\protected@file@percent }
\acronymused{VAE}
\citation{joy_capturing_2020}
\citation{narayanaswamy_learning_2017}
\citation{joy_capturing_2020}
\citation{nie_semi_supervised_2020}
\citation{adel_discovering_2018}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\citation{narayanaswamy_learning_2017}
\AC@undonewlabel{acro:CCVAE}
\newlabel{acro:CCVAE}{{4.3.1}{14}{Semantic Conflation Problem}{section*.10}{}}
\acronymused{CCVAE}
\acronymused{SSVAE}
\acronymused{CCVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Breaking the ELBO Bottleneck}{14}{subsubsection.4.3.2}\protected@file@percent }
\acronymused{SSVAE}
\acronymused{ELBO}
\AC@undonewlabel{acro:SHOT-VAE}
\newlabel{acro:SHOT-VAE}{{4.3.2}{14}{Breaking the ELBO Bottleneck}{section*.11}{}}
\acronymused{SHOT-VAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{ELBO}
\citation{feng_shot_vae_2021}
\citation{kingma_semi-supervised_2014}
\citation{shu_weakly_2019}
\citation{shu_weakly_2019}
\citation{narayanaswamy_learning_2017}
\citation{chen_isolating_2019}
\citation{locatello_challenging_2018}
\citation{shu_weakly_2019}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{locatello_weakly_2020}
\citation{joy_learning_2021}
\citation{locatello_challenging_2018}
\citation{lin_infogan_2020,bouchacourt_multilevel_2017,ke_apgvae_2023,Kim2018DisentanglingBF}
\citation{narayanaswamy_learning_2017}
\citation{Kim2018DisentanglingBF}
\citation{narayanaswamy_learning_2017}
\citation{perry2010continuous}
\acronymused{ELBO}
\acronymused{SHOT-VAE}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{SHOT-VAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Differentiating Consistency and Restrictiveness}{15}{subsubsection.4.3.3}\protected@file@percent }
\AC@undonewlabel{acro:MIG}
\newlabel{acro:MIG}{{4.3.3}{15}{Differentiating Consistency and Restrictiveness}{section*.12}{}}
\acronymused{MIG}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Extensions in Supervision: From Semi to Weak}{15}{subsection.4.4}\protected@file@percent }
\acronymused{VAE}
\citation{locatello_weakly_2020}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{chen_weakly_2020}
\citation{narayanaswamy_learning_2017}
\citation{locatello_weakly_2020}
\citation{locatello_weakly_2020}
\citation{chen_weakly_2020}
\citation{yang_disentangling_2019}
\citation{locatello_challenging_2018}
\citation{joy_learning_2021}
\citation{narayanaswamy_learning_2017}
\citation{joy_learning_2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}The Weak Supervision Paradigm}{16}{subsubsection.4.4.1}\protected@file@percent }
\newlabel{sec:ext_weak_paradigm}{{4.4.1}{16}{The Weak Supervision Paradigm}{subsubsection.4.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Multimodal VAEs and Mutual Supervision}{16}{subsubsection.4.4.2}\protected@file@percent }
\citation{locatello_weakly_2020}
\citation{narayanaswamy_learning_2017}
\citation{gordon_meta_2019,kulinski_explaining_2023}
\citation{narayanaswamy_learning_2017}
\citation{vaze_representation_2023}
\citation{narayanaswamy_learning_2017}
\citation{yang_usupervised_2019}
\citation{narayanaswamy_learning_2017}
\citation{yang_usupervised_2019}
\citation{narayanaswamy_learning_2017}
\citation{biswal_eva_2020}
\citation{li_multi_2018}
\acronymused{VAE}
\AC@undonewlabel{acro:PoE}
\newlabel{acro:PoE}{{4.4.2}{17}{Multimodal VAEs and Mutual Supervision}{section*.13}{}}
\acronymused{PoE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Applications and Integrations in the Literature}{17}{subsection.4.5}\protected@file@percent }
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Domain Adaptation in the Medical Domain}{17}{subsubsection.4.5.1}\protected@file@percent }
\acronymused{SSVAE}
\acronymused{SSVAE}
\citation{zhang_causal_2020}
\citation{narayanaswamy_learning_2017}
\citation{ye_learning_2020}
\citation{narayanaswamy_learning_2017}
\citation{ye_learning_2020}
\citation{kingma_semi-supervised_2014}
\citation{ye_lifelong_2022}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Causal Extension and Robustness}{18}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Catastrophic Forgetting and Generative Replay}{18}{subsubsection.4.5.3}\protected@file@percent }
\acronymused{VAE}
\AC@undonewlabel{acro:GAN}
\newlabel{acro:GAN}{{4.5.3}{18}{Catastrophic Forgetting and Generative Replay}{section*.14}{}}
\acronymused{GAN}
\acronymused{SSVAE}
\AC@undonewlabel{acro:M1+M2}
\newlabel{acro:M1+M2}{{4.5.3}{18}{Catastrophic Forgetting and Generative Replay}{section*.15}{}}
\acronymused{M1+M2}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\citation{deng2012mnist}
\citation{narayanaswamy_learning_2017}
\citation{agarap2018deep}
\citation{narayanaswamy_learning_2017}
\citation{kingma2017adammethodstochasticoptimization}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{Higgins2016betaVAELB}
\citation{Kim2018DisentanglingBF}
\citation{chen_isolating_2019}
\citation{Higgins2016betaVAELB}
\citation{Kim2018DisentanglingBF}
\citation{chen_isolating_2019}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{19}{section.5}\protected@file@percent }
\newlabel{experiments}{{5}{19}{Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Implementation Details}{19}{subsection.5.1}\protected@file@percent }
\newlabel{sec:implementation_details}{{5.1}{19}{Implementation Details}{subsection.5.1}{}}
\acronymused{MNIST}
\AC@undonewlabel{acro:ReLU}
\newlabel{acro:ReLU}{{5.1}{19}{Implementation Details}{section*.16}{}}
\acronymused{ReLU}
\acronymused{SSVAE}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \ac {SSVAE} architecture used throughout all experiments. The network has $28*28=784$ inputs matching the \ac {MNIST} input, 256 hidden neurons, 10 style variables and 10 digit neurons.}}{19}{figure.caption.17}\protected@file@percent }
\acronymused{SSVAE}
\acronymused{MNIST}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:architecture}{{1}{19}{\ac {SSVAE} architecture used throughout all experiments. The network has $28*28=784$ inputs matching the \ac {MNIST} input, 256 hidden neurons, 10 style variables and 10 digit neurons}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Supervision Weight}{19}{subsection.5.2}\protected@file@percent }
\newlabel{sec:alpha_exp}{{5.2}{19}{Supervision Weight}{subsection.5.2}{}}
\acronymused{MNIST}
\acronymused{MNIST}
\acronymused{SSVAE}
\citation{lin_infogan_2020,bouchacourt_multilevel_2017,ke_apgvae_2023,Kim2018DisentanglingBF}
\citation{Higgins2016betaVAELB}
\citation{Kim2018DisentanglingBF}
\citation{chen_isolating_2019}
\citation{Higgins2016betaVAELB}
\citation{Kim2018DisentanglingBF}
\citation{chen_isolating_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Supervision Weight} experiment results with varying $\alpha $ for supervised set sizes of 100, 600, 1000 and 3000. Error bars indicate variance across 10 random seeds. \textbf  {Top Left:} final test \ac {ELBO} for varying $\alpha $ remains relatively stable. \textbf  {Bottom Left:} final test accuracy of $y$ improves for higher values of $\alpha $. \textbf  {Right:} disentanglement metrics ($\beta $-VAE \cite  {Higgins2016betaVAELB}, Factor-VAE \cite  {Kim2018DisentanglingBF}, Mutual Information Gap \cite  {chen_isolating_2019}) decreases with increasing $\alpha $ across all three metrics.}}{20}{figure.caption.18}\protected@file@percent }
\acronymused{ELBO}
\newlabel{fig:alpha_exp}{{2}{20}{\textbf {Supervision Weight} experiment results with varying $\alpha $ for supervised set sizes of 100, 600, 1000 and 3000. Error bars indicate variance across 10 random seeds. \textbf {Top Left:} final test \ac {ELBO} for varying $\alpha $ remains relatively stable. \textbf {Bottom Left:} final test accuracy of $y$ improves for higher values of $\alpha $. \textbf {Right:} disentanglement metrics ($\beta $-VAE \cite {Higgins2016betaVAELB}, Factor-VAE \cite {Kim2018DisentanglingBF}, Mutual Information Gap \cite {chen_isolating_2019}) decreases with increasing $\alpha $ across all three metrics}{figure.caption.18}{}}
\acronymused{ELBO}
\acronymused{ELBO}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Label Corruption}{20}{subsection.5.3}\protected@file@percent }
\newlabel{sec:noise_exp}{{5.3}{20}{Label Corruption}{subsection.5.3}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Label Corruption} experiment results with varying label corruption rate for supervised set sizes of 100, 600, 1000 and 3000. Error bars indicate variance across 10 random seeds. \textbf  {Top Left:} final test \ac {ELBO} for varying noise levels remains relatively stable. \textbf  {Bottom Left:} while the final test accuracy of $y$ decreases for higher corruption rates it remains relatively high for up to 20\% noise. \textbf  {Right:} disentanglement (as measured through $\beta $-VAE \cite  {Higgins2016betaVAELB}, Factor-VAE \cite  {Kim2018DisentanglingBF}, Mutual Information Gap \cite  {chen_isolating_2019}) increase with a higher corruption rate across all three metrics.}}{21}{figure.caption.19}\protected@file@percent }
\acronymused{ELBO}
\newlabel{fig:noise_exp}{{3}{21}{\textbf {Label Corruption} experiment results with varying label corruption rate for supervised set sizes of 100, 600, 1000 and 3000. Error bars indicate variance across 10 random seeds. \textbf {Top Left:} final test \ac {ELBO} for varying noise levels remains relatively stable. \textbf {Bottom Left:} while the final test accuracy of $y$ decreases for higher corruption rates it remains relatively high for up to 20\% noise. \textbf {Right:} disentanglement (as measured through $\beta $-VAE \cite {Higgins2016betaVAELB}, Factor-VAE \cite {Kim2018DisentanglingBF}, Mutual Information Gap \cite {chen_isolating_2019}) increase with a higher corruption rate across all three metrics}{figure.caption.19}{}}
\acronymused{ELBO}
\acronymused{ELBO}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{22}{section.6}\protected@file@percent }
\newlabel{discussion}{{6}{22}{Discussion}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Performance and Compositionality}{22}{subsection.6.1}\protected@file@percent }
\acronymused{MNIST}
\acronymused{SVHN}
\acronymused{MNIST}
\acronymused{MNIST}
\acronymused{MNIST}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}The Role of Structure in Disentanglement}{22}{subsection.6.2}\protected@file@percent }
\acronymused{SVHN}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Limitations in Heterogeneous Sparse Supervision}{22}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Future Directions}{23}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{24}{section.7}\protected@file@percent }
\newlabel{conclusion}{{7}{24}{Conclusion}{section.7}{}}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {A}Systematic Review Process}{V}{appendix.A}\protected@file@percent }
\newlabel{apx:systematic_review}{{A}{V}{Systematic Review Process}{appendix.A}{}}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {section}{\numberline {B}SSVAE Training}{V}{appendix.B}\protected@file@percent }
\newlabel{apx:training}{{B}{V}{SSVAE Training}{appendix.B}{}}
\newlabel{fig:train_alpha}{{4a}{V}{All training runs of the \textbf {supervision factor $\pmb \alpha $ experiment} in \autoref {sec:alpha_exp}}{figure.caption.20}{}}
\newlabel{sub@fig:train_alpha}{{a}{V}{All training runs of the \textbf {supervision factor $\pmb \alpha $ experiment} in \autoref {sec:alpha_exp}}{figure.caption.20}{}}
\newlabel{fig:train_noise}{{4b}{V}{All training runs of the \textbf {corruption rate experiment} in \autoref {sec:noise_exp}}{figure.caption.20}{}}
\newlabel{sub@fig:train_noise}{{b}{V}{All training runs of the \textbf {corruption rate experiment} in \autoref {sec:noise_exp}}{figure.caption.20}{}}
\bibdata{bibliography.bib}
\bibstyle{dcu}
\gdef \@abspage@last{30}
