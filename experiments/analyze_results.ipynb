{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273de191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25554c",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2546099",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = '../results'\n",
    "\n",
    "# Load MNIST results\n",
    "mnist_sparsity = None\n",
    "mnist_noise = None\n",
    "\n",
    "if os.path.exists(os.path.join(results_path, 'MNIST_sparsity_results.json')):\n",
    "    with open(os.path.join(results_path, 'MNIST_sparsity_results.json'), 'r') as f:\n",
    "        mnist_sparsity = json.load(f)\n",
    "\n",
    "if os.path.exists(os.path.join(results_path, 'MNIST_noise_results.json')):\n",
    "    with open(os.path.join(results_path, 'MNIST_noise_results.json'), 'r') as f:\n",
    "        mnist_noise = json.load(f)\n",
    "\n",
    "# Load FashionMNIST results\n",
    "fashion_sparsity = None\n",
    "fashion_noise = None\n",
    "\n",
    "if os.path.exists(os.path.join(results_path, 'FashionMNIST_sparsity_results.json')):\n",
    "    with open(os.path.join(results_path, 'FashionMNIST_sparsity_results.json'), 'r') as f:\n",
    "        fashion_sparsity = json.load(f)\n",
    "\n",
    "if os.path.exists(os.path.join(results_path, 'FashionMNIST_noise_results.json')):\n",
    "    with open(os.path.join(results_path, 'FashionMNIST_noise_results.json'), 'r') as f:\n",
    "        fashion_noise = json.load(f)\n",
    "\n",
    "print(\"Loaded results:\")\n",
    "if mnist_sparsity:\n",
    "    print(f\"  MNIST Sparsity: {len(mnist_sparsity)} experiments\")\n",
    "if mnist_noise:\n",
    "    print(f\"  MNIST Noise: {len(mnist_noise)} experiments\")\n",
    "if fashion_sparsity:\n",
    "    print(f\"  FashionMNIST Sparsity: {len(fashion_sparsity)} experiments\")\n",
    "if fashion_noise:\n",
    "    print(f\"  FashionMNIST Noise: {len(fashion_noise)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d488be",
   "metadata": {},
   "source": [
    "## Label Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd86463",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mnist_sparsity or fashion_sparsity:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    datasets = []\n",
    "    if mnist_sparsity:\n",
    "        datasets.append(('MNIST', mnist_sparsity))\n",
    "    if fashion_sparsity:\n",
    "        datasets.append(('FashionMNIST', fashion_sparsity))\n",
    "    \n",
    "    for dataset_name, results in datasets:\n",
    "        label_fracs = [r['label_fraction'] * 100 for r in results]\n",
    "        accuracies = [r['final_test_accuracy'] for r in results]\n",
    "        beta_scores = [r['disentanglement_metrics']['beta_vae'] for r in results]\n",
    "        mig_scores = [r['disentanglement_metrics']['mig'] for r in results]\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0].plot(label_fracs, accuracies, 'o-', label=dataset_name, linewidth=2, markersize=8)\n",
    "        axes[0].set_xlabel('Label Fraction (%)', fontsize=12)\n",
    "        axes[0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "        axes[0].set_title('Classification Accuracy vs Label Sparsity', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xscale('log')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend(fontsize=10)\n",
    "        \n",
    "        # Beta-VAE\n",
    "        axes[1].plot(label_fracs, beta_scores, 's-', label=dataset_name, linewidth=2, markersize=8)\n",
    "        axes[1].set_xlabel('Label Fraction (%)', fontsize=12)\n",
    "        axes[1].set_ylabel('Beta-VAE Score', fontsize=12)\n",
    "        axes[1].set_title('Disentanglement (Beta-VAE) vs Label Sparsity', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xscale('log')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend(fontsize=10)\n",
    "        \n",
    "        # MIG\n",
    "        axes[2].plot(label_fracs, mig_scores, '^-', label=dataset_name, linewidth=2, markersize=8)\n",
    "        axes[2].set_xlabel('Label Fraction (%)', fontsize=12)\n",
    "        axes[2].set_ylabel('MIG Score', fontsize=12)\n",
    "        axes[2].set_title('Mutual Information Gap vs Label Sparsity', fontsize=14, fontweight='bold')\n",
    "        axes[2].set_xscale('log')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_path, 'sparsity_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No sparsity results found. Run the experiment first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16c84e",
   "metadata": {},
   "source": [
    "## Label Noise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mnist_noise or fashion_noise:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    datasets = []\n",
    "    if mnist_noise:\n",
    "        datasets.append(('MNIST', mnist_noise))\n",
    "    if fashion_noise:\n",
    "        datasets.append(('FashionMNIST', fashion_noise))\n",
    "    \n",
    "    for dataset_name, results in datasets:\n",
    "        corruption_rates = [r['corruption_rate'] * 100 for r in results]\n",
    "        accuracies = [r['final_test_accuracy'] for r in results]\n",
    "        beta_scores = [r['disentanglement_metrics']['beta_vae'] for r in results]\n",
    "        mig_scores = [r['disentanglement_metrics']['mig'] for r in results]\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0].plot(corruption_rates, accuracies, 'o-', label=dataset_name, linewidth=2, markersize=8)\n",
    "        axes[0].set_xlabel('Label Corruption Rate (%)', fontsize=12)\n",
    "        axes[0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "        axes[0].set_title('Classification Accuracy vs Label Noise', fontsize=14, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend(fontsize=10)\n",
    "        \n",
    "        # Beta-VAE\n",
    "        axes[1].plot(corruption_rates, beta_scores, 's-', label=dataset_name, linewidth=2, markersize=8)\n",
    "        axes[1].set_xlabel('Label Corruption Rate (%)', fontsize=12)\n",
    "        axes[1].set_ylabel('Beta-VAE Score', fontsize=12)\n",
    "        axes[1].set_title('Disentanglement (Beta-VAE) vs Label Noise', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend(fontsize=10)\n",
    "        \n",
    "        # MIG\n",
    "        axes[2].plot(corruption_rates, mig_scores, '^-', label=dataset_name, linewidth=2, markersize=8)\n",
    "        axes[2].set_xlabel('Label Corruption Rate (%)', fontsize=12)\n",
    "        axes[2].set_ylabel('MIG Score', fontsize=12)\n",
    "        axes[2].set_title('Mutual Information Gap vs Label Noise', fontsize=14, fontweight='bold')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_path, 'noise_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No noise results found. Run the experiment first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5310b7",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bfddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(results, experiment_type):\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{experiment_type} Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for r in results:\n",
    "        if 'label_fraction' in r:\n",
    "            print(f\"\\nLabel Fraction: {r['label_fraction']*100:.2f}%\")\n",
    "        if 'corruption_rate' in r:\n",
    "            print(f\"Corruption Rate: {r['corruption_rate']*100:.2f}%\")\n",
    "        \n",
    "        print(f\"  Accuracy: {r['final_test_accuracy']:.4f}\")\n",
    "        print(f\"  ELBO: {r['final_test_elbo']:.4e}\")\n",
    "        print(f\"  Beta-VAE: {r['disentanglement_metrics']['beta_vae']:.4f}\")\n",
    "        print(f\"  Factor-VAE: {r['disentanglement_metrics']['factor_vae']:.4f}\")\n",
    "        print(f\"  MIG: {r['disentanglement_metrics']['mig']:.4f}\")\n",
    "\n",
    "# Print all summaries\n",
    "print_summary(mnist_sparsity, \"MNIST Label Sparsity\")\n",
    "print_summary(mnist_noise, \"MNIST Label Noise\")\n",
    "print_summary(fashion_sparsity, \"FashionMNIST Label Sparsity\")\n",
    "print_summary(fashion_noise, \"FashionMNIST Label Noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bb7ef5",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf30ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for label sparsity experiments\n",
    "if mnist_sparsity:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for r in mnist_sparsity:\n",
    "        label = f\"{r['label_fraction']*100:.1f}%\"\n",
    "        epochs = range(1, len(r['test_accuracies']) + 1)\n",
    "        \n",
    "        axes[0].plot(epochs, r['test_accuracies'], label=label, alpha=0.7)\n",
    "        axes[1].plot(epochs, r['test_elbos'], label=label, alpha=0.7)\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "    axes[0].set_title('MNIST: Accuracy During Training', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(title='Label %', fontsize=9)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Test ELBO', fontsize=12)\n",
    "    axes[1].set_title('MNIST: ELBO During Training', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(title='Label %', fontsize=9)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_path, 'mnist_training_curves.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52cc34e",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Questions to Answer:\n",
    "\n",
    "1. **At what label fraction does performance collapse?**\n",
    "   - Examine the sparsity plots above\n",
    "   - Look for sharp drops in accuracy and disentanglement\n",
    "\n",
    "2. **How much label noise can the model tolerate?**\n",
    "   - Check the noise plots\n",
    "   - Identify corruption rates where metrics significantly degrade\n",
    "\n",
    "3. **Is disentanglement more robust than accuracy?**\n",
    "   - Compare the relative changes in accuracy vs Beta-VAE scores\n",
    "   - Does disentanglement drop faster or slower than accuracy?\n",
    "\n",
    "4. **How do MNIST and FashionMNIST compare?**\n",
    "   - Is FashionMNIST more sensitive to label quality?\n",
    "   - Which dataset maintains better disentanglement under stress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40c3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
