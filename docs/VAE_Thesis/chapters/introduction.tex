% What are semi-supervised vae's and what problem do they solve? 

% Why are they still relevant now? Why is supervised disentanglement an important contribution? 

% How does semi supervised disentanglement work? 

% What are our contributions? 
%   Explanation of the method
%   Limitations and trends in the literature
%   Own experiments









% With their paper Learning Disentangled Representations with
% Semi-Supervised Deep Generative Models, \citet{narayanaswamy_learning_2017} bridge the gap between fully specified graphical models and unsupervised deep neural networks for generative inference. Their method allows users to specify relationships for a subset of variables, while the rest are learned in an unsupervised manner. 

% \section{Introduction: The Quest for Interpretable Latent Structures}
% The field of machine learning has long been driven by the aspiration to develop systems that not only perform distinct tasks with high accuracy but also "understand" the data they process in a human-interpretable manner. At the core of this aspiration lies the concept of disentangled representation learning. The fundamental hypothesis driving this domain is that the high-dimensional data we observe—whether it be the pixel values of an image, the waveform of an audio signal, or the chemical graph of a molecule—is generated by the interaction of a relatively small number of semantically meaningful, independent factors of variation. These factors might correspond to physical attributes like the rotation of an object, lighting conditions, or shape, or they might represent more abstract concepts such as the sentiment of a sentence or the biological activity of a protein.

% A model that successfully disentangles these factors learns a mapping from the observable space to a latent space where each dimension (or a specific subset of dimensions) corresponds to exactly one of these generative factors. This separation allows for robust generalization, improved interpretability, and the ability to perform controlled generation—altering one factor (e.g., hair color) without affecting others (e.g., face orientation).

% In 2017, the paper "Learning Disentangled Representations with Semi-Supervised Deep Generative Models" by Siddharth et al. marked a significant milestone in this journey. At a time when unsupervised learning (e.g., standard Variational Autoencoders or VAEs) and fully supervised learning were the dominant paradigms, Siddharth et al. proposed a principled, semi-supervised framework. They argued that purely unsupervised learning is often insufficient to guarantee the identification of semantic factors due to the inherent identifiability problems of latent variable models. Instead, they posited that leveraging a small amount of labeled data to "anchor" specific latent variables could provide the necessary inductive bias to separate interpretable factors from the vast, unspecified variations inherent in real-world data.

% This report provides an exhaustive, expert-level analysis of the trajectory of research initiated and influenced by this seminal work. We will dissect the architectural and theoretical contributions of Siddharth et al., and then rigorously examine the profound limitations identified by subsequent research—most notably the impossibility theorems of Locatello et al., the semantic conflation issues highlighted by Joy et al., and the probabilistic instabilities identified by Mattei and Frellsen. Furthermore, we will explore the vast landscape of extensions that have arisen to address these challenges, ranging from weakly supervised paradigms and lifelong learning architectures to multimodal integration and safety-critical uncertainty quantification.

% By synthesizing findings from over 30 distinct research avenues, this document aims to construct a comprehensive narrative of how semi-supervised deep generative models have evolved from a promising method for digit classification into a foundational technology for robust, interpretable, and adaptable artificial intelligence.

% \section{The Foundational Framework: Siddharth et al. (2017)}
% To appreciate the extensions and critiques, one must first establish a rigorous understanding of the baseline framework proposed by Siddharth et al. Their work addresses the limitations of standard Variational Autoencoders (VAEs) in disentangling interpretable concepts from nuisance variables.

% \subsection{The Variational Autoencoder Context}
% The standard VAE is a probabilistic generative model that assumes observations $x$ are generated from a latent variable $z$: $p(x,z)=p(x \mid z)p(z)$. The prior $p(z)$ is typically chosen to be a standard isotropic Gaussian, $\mathcal{N}(0,I)$. Inference is performed by learning an approximate posterior $q_\phi(z \mid x)$, parameterized by a neural network (the encoder), which maps observations to the parameters of a Gaussian distribution (mean $\mu$ and variance $\sigma^2$). The model is trained by maximizing the Evidence Lower Bound (ELBO):
% $$ \mathcal{L}_{\text{ELBO}}(x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{\text{KL}}(q_\phi(z|x) || p(z)) $$
% The first term is the reconstruction loss, ensuring the latent code retains sufficient information to generate the input. The second term is the Kullback-Leibler (KL) divergence, which regularizes the posterior to match the prior. In the context of disentanglement, the hope is that the independent dimensions of the isotropic Gaussian prior will force the encoder to align the independent factors of variation in the data with the axes of the latent space. This is often referred to as "factorizing" the latent distribution.

% \subsection{The Semi-Supervised Innovation}
% Siddharth et al. identified that while standard VAEs learn compressed representations, they rarely learn disentangled ones reliably without supervision. The isotropic prior is rotationally invariant; thus, any rotation of the true factors is equally probable under the prior, leading to entangled representations where a single latent dimension might encode a mixture of color and shape.

% To resolve this, Siddharth et al. proposed a general graphical model structure that explicitly partitions the latent space into two sets of variables:
% \begin{itemize}
%     \item $y$ (Specified Variables): Latent variables for which partial supervision (labels) is available. These typically correspond to semantically distinct concepts like digit identity in MNIST or face attributes in CelebA.
%     \item $z$ (Unspecified Variables): Latent variables that capture the remaining variability in the data—often referred to as "style," "nuance," or "intra-class variation" (e.g., handwriting slant, stroke width, or background lighting).
% \end{itemize}
% The generative process is defined as $p(x,y,z)=p(x \mid y,z)p(y)p(z)$. Crucially, $y$ and $z$ are assumed to be independent a priori.

% \subsection{The Objective Function and Inference}
% The core contribution lies in how the model is trained with mixed labeled and unlabeled data.
% \begin{itemize}
%     \item \textbf{Labeled Data:} When the label $y_{\text{true}}$ is present, the model maximizes the ELBO for the joint pair $(x,y_{\text{true}})$. The inference network $q_\phi(z \mid x,y)$ predicts the unspecified factors given the image and the known label.
%     \item \textbf{Unlabeled Data:} When $y$ is missing, it is treated as a latent variable. The model must marginalize over $y$. The inference network now includes a classifier component $q_\phi(y \mid x)$ which predicts the likely label, and a component $q_\phi(z \mid x,y)$ which infers the style for a given putative label.
% \end{itemize}
% The training objective becomes a weighted sum of the supervised and unsupervised losses. Siddharth et al. employed the Gumbel-Softmax relaxation (or Concrete distribution) to allow for backpropagation through the discrete samples of $y$ during the unsupervised phase. This technical choice was pivotal, as it allowed the classifier $q_\phi(y \mid x)$ to be trained end-to-end with the generative components, improving the classification accuracy by leveraging the generative structure of the data.

% \subsection{The Promise of Disentanglement}
% The theoretical promise of this approach was that by "anchoring" the specified factors $y$ with labels, the model would be forced to push all other variation into $z$. Because $y$ and $z$ are independent in the generative model, the learned representation should effectively disentangle the class identity (digit) from the style (handwriting). Empirically, Siddharth et al. demonstrated this on MNIST and SVHN, showing that they could generate images of a specific digit (fixing $y$) while varying the style (sampling $z$), or transfer the style of one image to another digit.

% However, this "anchoring" hypothesis rests on strong assumptions about the data and the model's ability to utilize the unsupervised capacity efficiently. As we will see in the subsequent sections, these assumptions became the primary target of rigorous theoretical critique in the years following the paper's publication.