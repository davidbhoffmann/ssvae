\relax 
\AC@reset@newl@bel
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newacro{CCVAE}[CCVAE]{Characteristic Capturing VAE}
\newacro{DLVM}[DLVM]{Deep Latent Variable Model}
\newacro{ELBO}[ELBO]{evidence lower bound}
\newacro{GPPVAE}[GPPVAE]{Gaussian Process Prior VAE}
\newacro{GAN}[GAN]{Generative Adversarial Network}
\newacro{i.i.d.}[i.i.d.]{independent and identically distributed}
\newacro{M1}[M1]{latent-feature discriminative model}
\newacro{M1+M2}[M1+M2]{stacked generative semi-supervised model}
\newacro{M2}[M2]{generative semi-supervised model}
\newacro{MIG}[MIG]{Mutual Information Gap}
\newacro{MNIST}[MNIST]{Modified National Institute of Standards and Technology}
\newacro{SVHN}[SVHN]{Street View House Numbers}
\newacro{PoE}[PoE]{Product-of-Experts}
\newacro{SHOT-VAE}[SHOT-VAE]{SmootH-ELBO Optimal InTerpolation VAE}
\newacro{SSVAE}[SSVAE]{Semi-Supervised VAE}
\newacro{VAE}[VAE]{Variational Auto-Encoder}
\newacro{ReLU}[ReLU]{Rectified Linear Units}
\citation{bengio2013representation}
\citation{kingma_auto-encoding_2013}
\citation{locatello_challenging_2018}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{intro}{{1}{1}{Introduction}{section.1}{}}
\AC@undonewlabel{acro:VAE}
\newlabel{acro:VAE}{{1}{1}{Introduction}{section*.2}{}}
\acronymused{VAE}
\AC@undonewlabel{acro:SSVAE}
\newlabel{acro:SSVAE}{{1}{1}{Introduction}{section*.3}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{VAE}
\acronymused{SSVAE}
\citation{kingma_auto-encoding_2013,rezende2014stochastic}
\citation{bengio2013representation}
\citation{Higgins2016betaVAELB}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Formulation}{2}{section.2}\protected@file@percent }
\newlabel{background}{{2}{2}{Background and Formulation}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Disentanglement in Variational Auto-Encoders}{2}{subsection.2.1}\protected@file@percent }
\acronymused{VAE}
\acronymused{VAE}
\AC@undonewlabel{acro:ELBO}
\newlabel{acro:ELBO}{{2.1}{2}{Disentanglement in Variational Auto-Encoders}{section*.4}{}}
\acronymused{ELBO}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Degrees of Supervision}{2}{subsection.2.2}\protected@file@percent }
\citation{kingma_semi-supervised_2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Semi-Supervised Variational Auto-Encoders}{3}{subsection.2.3}\protected@file@percent }
\acronymused{VAE}
\citation{kingma_semi-supervised_2014}
\acronymused{ELBO}
\newlabel{eq:sup_obj}{{6}{4}{Semi-Supervised Variational Auto-Encoders}{equation.6}{}}
\newlabel{eq:unsup_obj}{{9}{4}{Semi-Supervised Variational Auto-Encoders}{equation.9}{}}
\newlabel{eq:combined_obj}{{10}{4}{Semi-Supervised Variational Auto-Encoders}{equation.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Importance Sampling}{4}{subsection.2.4}\protected@file@percent }
\newlabel{sec:imp_sampling}{{2.4}{4}{Importance Sampling}{subsection.2.4}{}}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {3}Semi-Supervised Disentanglement with Arbitrary Dependencies}{6}{section.3}\protected@file@percent }
\newlabel{method}{{3}{6}{Semi-Supervised Disentanglement with Arbitrary Dependencies}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Generalised Variational Objective}{6}{subsection.3.1}\protected@file@percent }
\citation{narayanaswamy_learning_2017}
\newlabel{eq:expectation_est}{{18}{7}{The Generalised Variational Objective}{equation.18}{}}
\newlabel{eq:disc_est}{{19}{7}{The Generalised Variational Objective}{equation.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Relation to Kingma's M2 Model}{7}{subsection.3.2}\protected@file@percent }
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{deng2012mnist}
\citation{netzer2011reading}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Graphical Model and Stochastic Implementation}{8}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Experiments and Findings}{8}{subsection.3.4}\protected@file@percent }
\newlabel{sec:their_eperiments}{{3.4}{8}{Experiments and Findings}{subsection.3.4}{}}
\AC@undonewlabel{acro:MNIST}
\newlabel{acro:MNIST}{{3.4}{8}{Experiments and Findings}{section*.5}{}}
\acronymused{MNIST}
\AC@undonewlabel{acro:SVHN}
\newlabel{acro:SVHN}{{3.4}{8}{Experiments and Findings}{section*.6}{}}
\acronymused{SVHN}
\acronymused{MNIST}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Benchmark Classification: MNIST and SVHN}{8}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{sec:mnist}{{3.4.1}{8}{Benchmark Classification: MNIST and SVHN}{subsubsection.3.4.1}{}}
\acronymused{MNIST}
\acronymused{SVHN}
\acronymused{MNIST}
\acronymused{SVHN}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{GeBeKr01}
\citation{pmlr-v38-jampani15}
\citation{pmlr-v38-jampani15}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\acronymused{SSVAE}
\acronymused{MNIST}
\acronymused{SVHN}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Intrinsic Faces: Disentangling Continuous Factors}{9}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{sec:exp_faces}{{3.4.2}{9}{Intrinsic Faces: Disentangling Continuous Factors}{subsubsection.3.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Multi-MNIST: Stochastic Dimensionality and Compositionality}{9}{subsubsection.3.4.3}\protected@file@percent }
\acronymused{MNIST}
\acronymused{MNIST}
\acronymused{MNIST}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{GeBeKr01}
\citation{pmlr-v38-jampani15}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {4}Extensions and Limitations}{11}{section.4}\protected@file@percent }
\newlabel{extensions}{{4}{11}{Extensions and Limitations}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Own Criticism}{11}{subsection.4.1}\protected@file@percent }
\newlabel{sec:my_critique}{{4.1}{11}{Own Criticism}{subsection.4.1}{}}
\acronymused{SSVAE}
\newlabel{eq:yaleb}{{23}{11}{Own Criticism}{equation.23}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{pmlr-v38-jampani15}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{casale_gaussian_2018}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\acronymused{MNIST}
\acronymused{SVHN}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}General Critiques}{12}{subsection.4.2}\protected@file@percent }
\newlabel{sec:general_critique}{{4.2}{12}{General Critiques}{subsection.4.2}{}}
\AC@undonewlabel{acro:i.i.d.}
\newlabel{acro:i.i.d.}{{4.2}{12}{General Critiques}{section*.7}{}}
\acronymused{i.i.d.}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Impossibility of Unsupervised Disentanglement}{12}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{sec:impossibility_theorem}{{4.2.1}{12}{Impossibility of Unsupervised Disentanglement}{subsubsection.4.2.1}{}}
\acronymused{VAE}
\citation{locatello_challenging_2018}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{locatello_challenging_2018}
\citation{narayanaswamy_learning_2017}
\citation{casale_gaussian_2018}
\citation{narayanaswamy_learning_2017}
\citation{casale_gaussian_2018}
\citation{casale_gaussian_2018}
\citation{casale_gaussian_2018}
\citation{narayanaswamy_learning_2017}
\citation{esmaeili_structured_2018}
\acronymused{ELBO}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}The Limitation of Isotropic Priors}{13}{subsubsection.4.2.2}\protected@file@percent }
\acronymused{i.i.d.}
\AC@undonewlabel{acro:GPPVAE}
\newlabel{acro:GPPVAE}{{4.2.2}{13}{The Limitation of Isotropic Priors}{section*.8}{}}
\acronymused{GPPVAE}
\citation{mattei_leveraging_2018}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{mattei_leveraging_2018}
\citation{narayanaswamy_learning_2017}
\citation{joy_capturing_2020}
\acronymused{ELBO}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Unbounded Likelihoods and Mode Collapse}{14}{subsubsection.4.2.3}\protected@file@percent }
\AC@undonewlabel{acro:DLVM}
\newlabel{acro:DLVM}{{4.2.3}{14}{Unbounded Likelihoods and Mode Collapse}{section*.9}{}}
\acronymused{DLVM}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Disentanglement Implications and Limitations}{14}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Semantic Conflation Problem}{14}{subsubsection.4.3.1}\protected@file@percent }
\newlabel{sec:semantic_conflation}{{4.3.1}{14}{Semantic Conflation Problem}{subsubsection.4.3.1}{}}
\citation{joy_capturing_2020}
\citation{joy_capturing_2020}
\citation{narayanaswamy_learning_2017}
\citation{joy_capturing_2020}
\citation{nie_semi_supervised_2020}
\citation{adel_discovering_2018}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\acronymused{VAE}
\AC@undonewlabel{acro:CCVAE}
\newlabel{acro:CCVAE}{{4.3.1}{15}{Semantic Conflation Problem}{section*.10}{}}
\acronymused{CCVAE}
\acronymused{SSVAE}
\acronymused{CCVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Breaking the ELBO Bottleneck}{15}{subsubsection.4.3.2}\protected@file@percent }
\acronymused{SSVAE}
\acronymused{ELBO}
\AC@undonewlabel{acro:SHOT-VAE}
\newlabel{acro:SHOT-VAE}{{4.3.2}{15}{Breaking the ELBO Bottleneck}{section*.11}{}}
\acronymused{SHOT-VAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{ELBO}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\citation{kingma_semi-supervised_2014}
\citation{shu_weakly_2019}
\citation{shu_weakly_2019}
\citation{narayanaswamy_learning_2017}
\citation{chen_isolating_2019}
\citation{locatello_challenging_2018}
\citation{shu_weakly_2019}
\citation{narayanaswamy_learning_2017}
\acronymused{ELBO}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{SHOT-VAE}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{SHOT-VAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Differentiating Consistency and Restrictiveness}{16}{subsubsection.4.3.3}\protected@file@percent }
\AC@undonewlabel{acro:MIG}
\newlabel{acro:MIG}{{4.3.3}{16}{Differentiating Consistency and Restrictiveness}{section*.12}{}}
\acronymused{MIG}
\citation{locatello_challenging_2018}
\citation{locatello_weakly_2020}
\citation{joy_learning_2021}
\citation{locatello_challenging_2018}
\citation{lin_infogan_2020,bouchacourt_multilevel_2017,ke_apgvae_2023,Kim2018DisentanglingBF}
\citation{narayanaswamy_learning_2017}
\citation{Kim2018DisentanglingBF}
\citation{narayanaswamy_learning_2017}
\citation{perry2010continuous}
\citation{locatello_weakly_2020}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{chen_weakly_2020}
\citation{narayanaswamy_learning_2017}
\citation{locatello_weakly_2020}
\citation{locatello_weakly_2020}
\citation{chen_weakly_2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Extensions in Supervision: From Semi to Weak}{17}{subsection.4.4}\protected@file@percent }
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}The Weak Supervision Paradigm}{17}{subsubsection.4.4.1}\protected@file@percent }
\newlabel{sec:ext_weak_paradigm}{{4.4.1}{17}{The Weak Supervision Paradigm}{subsubsection.4.4.1}{}}
\citation{yang_disentangling_2019}
\citation{locatello_challenging_2018}
\citation{joy_learning_2021}
\citation{narayanaswamy_learning_2017}
\citation{joy_learning_2021}
\citation{locatello_weakly_2020}
\citation{narayanaswamy_learning_2017}
\citation{gordon_meta_2019,kulinski_explaining_2023}
\citation{narayanaswamy_learning_2017}
\citation{vaze_representation_2023}
\citation{narayanaswamy_learning_2017}
\citation{yang_usupervised_2019}
\citation{narayanaswamy_learning_2017}
\citation{yang_usupervised_2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Multimodal VAEs and Mutual Supervision}{18}{subsubsection.4.4.2}\protected@file@percent }
\acronymused{VAE}
\AC@undonewlabel{acro:PoE}
\newlabel{acro:PoE}{{4.4.2}{18}{Multimodal VAEs and Mutual Supervision}{section*.13}{}}
\acronymused{PoE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Applications and Integrations in the Literature}{18}{subsection.4.5}\protected@file@percent }
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Domain Adaptation in the Medical Domain}{18}{subsubsection.4.5.1}\protected@file@percent }
\acronymused{SSVAE}
\citation{narayanaswamy_learning_2017}
\citation{biswal_eva_2020}
\citation{li_multi_2018}
\citation{zhang_causal_2020}
\citation{narayanaswamy_learning_2017}
\citation{ye_learning_2020}
\citation{narayanaswamy_learning_2017}
\citation{ye_learning_2020}
\citation{kingma_semi-supervised_2014}
\acronymused{SSVAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Causal Extension and Robustness}{19}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Catastrophic Forgetting and Generative Replay}{19}{subsubsection.4.5.3}\protected@file@percent }
\acronymused{VAE}
\AC@undonewlabel{acro:GAN}
\newlabel{acro:GAN}{{4.5.3}{19}{Catastrophic Forgetting and Generative Replay}{section*.14}{}}
\acronymused{GAN}
\citation{ye_lifelong_2022}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\acronymused{SSVAE}
\AC@undonewlabel{acro:M1+M2}
\newlabel{acro:M1+M2}{{4.5.3}{20}{Catastrophic Forgetting and Generative Replay}{section*.15}{}}
\acronymused{M1+M2}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\citation{deng2012mnist}
\citation{narayanaswamy_learning_2017}
\citation{agarap2018deep}
\citation{narayanaswamy_learning_2017}
\citation{kingma2017adammethodstochasticoptimization}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{21}{section.5}\protected@file@percent }
\newlabel{experiments}{{5}{21}{Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Implementation Details}{21}{subsection.5.1}\protected@file@percent }
\newlabel{sec:implementation_details}{{5.1}{21}{Implementation Details}{subsection.5.1}{}}
\acronymused{MNIST}
\AC@undonewlabel{acro:ReLU}
\newlabel{acro:ReLU}{{5.1}{21}{Implementation Details}{section*.16}{}}
\acronymused{ReLU}
\acronymused{SSVAE}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \ac {SSVAE} architecture used throughout all experiments. The network has $28*28=784$ inputs matching the \ac {MNIST} input, 256 hidden neurons, 10 style variables and 10 digit neurons.}}{21}{figure.caption.17}\protected@file@percent }
\acronymused{SSVAE}
\acronymused{MNIST}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:architecture}{{1}{21}{\ac {SSVAE} architecture used throughout all experiments. The network has $28*28=784$ inputs matching the \ac {MNIST} input, 256 hidden neurons, 10 style variables and 10 digit neurons}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Supervision Weight}{21}{subsection.5.2}\protected@file@percent }
\newlabel{sec:alpha_exp}{{5.2}{21}{Supervision Weight}{subsection.5.2}{}}
\acronymused{MNIST}
\acronymused{MNIST}
\citation{Higgins2016betaVAELB}
\citation{Kim2018DisentanglingBF}
\citation{chen_isolating_2019}
\citation{Higgins2016betaVAELB}
\citation{Kim2018DisentanglingBF}
\citation{chen_isolating_2019}
\citation{lin_infogan_2020,bouchacourt_multilevel_2017,ke_apgvae_2023,Kim2018DisentanglingBF}
\citation{narayanaswamy_learning_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Supervision Weight} experiment results with varying $\alpha $ for supervised set sizes of 100, 600, 1000 and 3000. Error bars indicate variance across 10 random seeds. \textbf  {Top Left:} final test \ac {ELBO} for varying $\alpha $ remains relatively stable. \textbf  {Bottom Left:} final test accuracy of $y$ improves for higher values of $\alpha $. \textbf  {Right:} disentanglement metrics ($\beta $-VAE \cite  {Higgins2016betaVAELB}, Factor-VAE \cite  {Kim2018DisentanglingBF}, Mutual Information Gap \cite  {chen_isolating_2019}) decreases with increasing $\alpha $ across all three metrics.}}{22}{figure.caption.18}\protected@file@percent }
\acronymused{ELBO}
\newlabel{fig:alpha_exp}{{2}{22}{\textbf {Supervision Weight} experiment results with varying $\alpha $ for supervised set sizes of 100, 600, 1000 and 3000. Error bars indicate variance across 10 random seeds. \textbf {Top Left:} final test \ac {ELBO} for varying $\alpha $ remains relatively stable. \textbf {Bottom Left:} final test accuracy of $y$ improves for higher values of $\alpha $. \textbf {Right:} disentanglement metrics ($\beta $-VAE \cite {Higgins2016betaVAELB}, Factor-VAE \cite {Kim2018DisentanglingBF}, Mutual Information Gap \cite {chen_isolating_2019}) decreases with increasing $\alpha $ across all three metrics}{figure.caption.18}{}}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{ELBO}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Label Corruption}{22}{subsection.5.3}\protected@file@percent }
\newlabel{sec:noise_exp}{{5.3}{22}{Label Corruption}{subsection.5.3}{}}
\acronymused{SSVAE}
\citation{Higgins2016betaVAELB}
\citation{Kim2018DisentanglingBF}
\citation{chen_isolating_2019}
\citation{Higgins2016betaVAELB}
\citation{Kim2018DisentanglingBF}
\citation{chen_isolating_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Label Corruption} experiment results with varying label corruption rate for supervised set sizes of 100, 600, 1000 and 3000. Error bars indicate variance across 10 random seeds. \textbf  {Top Left:} final test \ac {ELBO} for varying noise levels remains relatively stable. \textbf  {Bottom Left:} while the final test accuracy of $y$ decreases for higher corruption rates it remains relatively high for up to 20\% noise. \textbf  {Right:} disentanglement (as measured through $\beta $-VAE \cite  {Higgins2016betaVAELB}, Factor-VAE \cite  {Kim2018DisentanglingBF}, Mutual Information Gap \cite  {chen_isolating_2019}) increase with a higher corruption rate across all three metrics.}}{23}{figure.caption.19}\protected@file@percent }
\acronymused{ELBO}
\newlabel{fig:noise_exp}{{3}{23}{\textbf {Label Corruption} experiment results with varying label corruption rate for supervised set sizes of 100, 600, 1000 and 3000. Error bars indicate variance across 10 random seeds. \textbf {Top Left:} final test \ac {ELBO} for varying noise levels remains relatively stable. \textbf {Bottom Left:} while the final test accuracy of $y$ decreases for higher corruption rates it remains relatively high for up to 20\% noise. \textbf {Right:} disentanglement (as measured through $\beta $-VAE \cite {Higgins2016betaVAELB}, Factor-VAE \cite {Kim2018DisentanglingBF}, Mutual Information Gap \cite {chen_isolating_2019}) increase with a higher corruption rate across all three metrics}{figure.caption.19}{}}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{ELBO}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{lin_infogan_2020,bouchacourt_multilevel_2017,ke_apgvae_2023,Kim2018DisentanglingBF}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{24}{section.6}\protected@file@percent }
\newlabel{discussion}{{6}{24}{Discussion}{section.6}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{SSVAE}
\acronymused{MNIST}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{joy_learning_2021}
\citation{locatello_weakly_2020}
\citation{zhang_causal_2020}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{25}{section.7}\protected@file@percent }
\newlabel{conclusion}{{7}{25}{Conclusion}{section.7}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {A}Systematic Review Process}{V}{appendix.A}\protected@file@percent }
\newlabel{apx:systematic_review}{{A}{V}{Systematic Review Process}{appendix.A}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {section}{\numberline {B}SSVAE Training}{V}{appendix.B}\protected@file@percent }
\newlabel{apx:training}{{B}{V}{SSVAE Training}{appendix.B}{}}
\newlabel{fig:train_alpha}{{4a}{V}{All training runs of the \textbf {supervision factor $\pmb \alpha $ experiment} in \autoref {sec:alpha_exp}}{figure.caption.20}{}}
\newlabel{sub@fig:train_alpha}{{a}{V}{All training runs of the \textbf {supervision factor $\pmb \alpha $ experiment} in \autoref {sec:alpha_exp}}{figure.caption.20}{}}
\newlabel{fig:train_noise}{{4b}{V}{All training runs of the \textbf {corruption rate experiment} in \autoref {sec:noise_exp}}{figure.caption.20}{}}
\newlabel{sub@fig:train_noise}{{b}{V}{All training runs of the \textbf {corruption rate experiment} in \autoref {sec:noise_exp}}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Supervision Weight Results}{VI}{appendix.C}\protected@file@percent }
\newlabel{sec:results_alpha_exp}{{C}{VI}{Supervision Weight Results}{appendix.C}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Aggregated results for the supervision weight experiment.}}{VI}{table.caption.21}\protected@file@percent }
\newlabel{tab:alpha_exp}{{1}{VI}{Aggregated results for the supervision weight experiment}{table.caption.21}{}}
\gdef \LT@i {\LT@entry 
    {6}{44.63857pt}\LT@entry 
    {1}{58.89954pt}\LT@entry 
    {1}{35.49977pt}\LT@entry 
    {1}{64.20921pt}\LT@entry 
    {1}{73.02162pt}\LT@entry 
    {5}{32.88869pt}\LT@entry 
    {1}{70.64526pt}\LT@entry 
    {1}{85.49786pt}\LT@entry 
    {1}{0.0pt}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Detailed results for the supervision weight experiment.}}{XIV}{table.2}\protected@file@percent }
\newlabel{tab:full_alpha_exp}{{2}{XIV}{Detailed results for the supervision weight experiment}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Corruption Rate Results}{XV}{appendix.D}\protected@file@percent }
\newlabel{sec:results_corruption_exp}{{D}{XV}{Corruption Rate Results}{appendix.D}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Aggregated results for the corruption rate experiment.}}{XV}{table.caption.22}\protected@file@percent }
\newlabel{tab:corruption_exp}{{3}{XV}{Aggregated results for the corruption rate experiment}{table.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Detailed results for the corruption rate experiment.}}{XV}{table.4}\protected@file@percent }
\newlabel{tab:full_corruption_exp}{{4}{XV}{Detailed results for the corruption rate experiment}{table.4}{}}
\gdef \LT@ii {\LT@entry 
    {1}{53.51208pt}\LT@entry 
    {2}{58.89954pt}\LT@entry 
    {1}{35.49977pt}\LT@entry 
    {2}{64.20921pt}\LT@entry 
    {2}{73.02162pt}\LT@entry 
    {1}{34.26646pt}\LT@entry 
    {2}{70.64526pt}\LT@entry 
    {2}{85.49786pt}\LT@entry 
    {1}{0.0pt}}
\bibdata{bibliography}
\bibcite{adel_discovering_2018}{{1}{2018}{{Adel et~al.}}{{Adel, Ghahramani and\ Weller}}}
\bibcite{agarap2018deep}{{2}{2018}{{Agarap}}{{}}}
\bibcite{bengio2013representation}{{3}{2013}{{Bengio et~al.}}{{Bengio, Courville and\ Vincent}}}
\bibcite{biswal_eva_2020}{{4}{2021}{{Biswal et~al.}}{{Biswal, Ghosh, Duke, Malin, Stewart, Xiao and\ Sun}}}
\bibcite{bouchacourt_multilevel_2017}{{5}{2018}{{Bouchacourt et~al.}}{{Bouchacourt, Tomioka and\ Nowozin}}}
\bibcite{casale_gaussian_2018}{{6}{2018}{{Casale et~al.}}{{Casale, Dalca, Saglietti, Listgarten and\ Fusi}}}
\bibcite{chen_weakly_2020}{{7}{2020}{{Chen and\ Batmanghelich}}{{}}}
\bibcite{chen_isolating_2019}{{8}{2018}{{Chen et~al.}}{{Chen, Li, Grosse and\ Duvenaud}}}
\bibcite{deng2012mnist}{{9}{2012}{{Deng}}{{}}}
\bibcite{esmaeili_structured_2018}{{10}{2019}{{Esmaeili et~al.}}{{Esmaeili, Wu, Jain, Bozkurt, Siddharth, Paige, Brooks, Dy and\ Meent}}}
\bibcite{feng_shot_vae_2021}{{11}{2021}{{Feng et~al.}}{{Feng, Kong, Chen, Zhang, Zhu and\ Chen}}}
\bibcite{GeBeKr01}{{12}{2001}{{Georghiades et~al.}}{{Georghiades, Belhumeur and\ Kriegman}}}
\bibcite{gordon_meta_2019}{{13}{2019}{{Gordon et~al.}}{{Gordon, Bronskill, Bauer, Nowozin and\ Turner}}}
\bibcite{Higgins2016betaVAELB}{{14}{2017}{{Higgins et~al.}}{{Higgins, Matthey, Pal, Burgess, Glorot, Botvinick, Mohamed and\ Lerchner}}}
\bibcite{pmlr-v38-jampani15}{{15}{2015}{{Jampani et~al.}}{{Jampani, Eslami, Tarlow, Kohli and\ Winn}}}
\bibcite{joy_capturing_2020}{{16}{2021}{{Joy et~al.}}{{Joy, Schmon, Torr, Siddharth and\ Rainforth}}}
\bibcite{joy_learning_2021}{{17}{2022}{{Joy et~al.}}{{Joy, Shi, Torr, Rainforth, Schmon and\ Siddharth}}}
\bibcite{ke_apgvae_2023}{{18}{2024}{{Ke et~al.}}{{Ke, Jing, Wo\'{z}niak, Xu, Liang and\ Zheng}}}
\bibcite{Kim2018DisentanglingBF}{{19}{2018}{{Kim and\ Mnih}}{{}}}
\bibcite{kingma2017adammethodstochasticoptimization}{{20}{2015}{{Kingma and\ Ba}}{{}}}
\bibcite{kingma_semi-supervised_2014}{{21}{2014}{{Kingma et~al.}}{{Kingma, Rezende, Mohamed and\ Welling}}}
\bibcite{kingma_auto-encoding_2013}{{22}{2014}{{Kingma and\ Welling}}{{}}}
\bibcite{kulinski_explaining_2023}{{23}{2023}{{Kulinski and\ Inouye}}{{}}}
\bibcite{li_multi_2018}{{24}{2018}{{Li et~al.}}{{Li, Zhang and\ Liu}}}
\bibcite{lin_infogan_2020}{{25}{2020}{{Lin et~al.}}{{Lin, Thekumparampil, Fanti and\ Oh}}}
\bibcite{locatello_challenging_2018}{{26}{2019}{{Locatello et~al.}}{{Locatello, Bauer, Lucic, Gelly, Schoelkopf and\ Bachem}}}
\bibcite{locatello_weakly_2020}{{27}{2020}{{Locatello et~al.}}{{Locatello, Poole, Raetsch, Schoelkopf, Bachem and\ Tschannen}}}
\bibcite{mattei_leveraging_2018}{{28}{2018}{{Mattei and\ Frellsen}}{{}}}
\bibcite{narayanaswamy_learning_2017}{{29}{2017}{{Narayanaswamy et~al.}}{{Narayanaswamy, Paige, van~de Meent, Desmaison, Goodman, Kohli, Wood and\ Torr}}}
\bibcite{netzer2011reading}{{30}{2011}{{Netzer et~al.}}{{Netzer, Wang, Coates, Bissacco, Wu, Ng et~al.}}}
\bibcite{nie_semi_supervised_2020}{{31}{2020}{{Nie et~al.}}{{Nie, Karras, Garg, Debnath, Patney, Patel and\ Anandkumar}}}
\bibcite{perry2010continuous}{{32}{2010}{{Perry et~al.}}{{Perry, Rolls and\ Stringer}}}
\bibcite{rezende2014stochastic}{{33}{2014}{{Rezende et~al.}}{{Rezende, Mohamed and\ Wierstra}}}
\bibcite{shu_weakly_2019}{{34}{2020}{{Shu et~al.}}{{Shu, Chen, Kumar, Ermon and\ Poole}}}
\bibcite{vaze_representation_2023}{{35}{2023}{{Vaze et~al.}}{{Vaze, Vedaldi and\ Zisserman}}}
\bibcite{yang_usupervised_2019}{{36}{2019}{{Yang et~al.}}{{Yang, Dvornek, Zhang, Chapiro, Lin and\ Duncan}}}
\bibcite{yang_disentangling_2019}{{37}{2019}{{Yang and\ Yao}}{{}}}
\bibcite{ye_learning_2020}{{38}{2020}{{Ye and\ Bors}}{{}}}
\bibcite{ye_lifelong_2022}{{39}{2022}{{Ye and\ Bors}}{{}}}
\bibcite{zhang_causal_2020}{{40}{2020}{{Zhang et~al.}}{{Zhang, Zhang and\ Li}}}
\bibstyle{dcu}
\gdef \@abspage@last{51}
