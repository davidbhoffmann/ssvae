% Academic Ancestors of our work, i.e. all concepts and prior work that are required for understanding our method. 
% Usually includes a subsection, Problem Setting, which formally introduces the problem setting and notation (Formalism) for our method. Highlights any specific assumptions that are made that are unusual. 

\subsection{Disentanglement in Variational Auto-Encoders}

\acp{VAE} are generative models that learn a deep latent variable model by maximizing a lower bound on the marginal likelihood of the data \citep{kingma_auto-encoding_2013, rezende2014stochastic}. Given a dataset $\mathcal{D} = \{x^{(i)}\}_{i=1}^N$, we assume the data is generated by a random process involving an unobserved continuous random variable $z$. The generative process consists of a prior distribution $p_\theta(z)$ and a conditional likelihood $p_\theta(x|z)$, typically parametrised by a neural network (the decoder) with parameters $\theta$.

Since the true posterior $p_\theta(z|x)$ is generally intractable, \acp{VAE} introduce an approximate posterior $q_\phi(z|x)$, parametrised by a separate neural network (the encoder) with parameters $\phi$. The model is trained by maximizing the \ac{ELBO} on the marginal log-likelihood $\log p_\theta(x)$:

\begin{equation}
    \log p_\theta(x) \geq \mathcal{L}_{\text{ELBO}}(x; \theta, \phi) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p(z))
\end{equation}

The first term is the reconstruction error, encouraging the decoder to recover the data from the latent code. The second term is the Kullback-Leibler (KL) divergence, which regularizes the approximate posterior to be close to the prior, typically assumed to be a standard multivariate Gaussian $p(z) = \mathcal{N}(0, I)$.

To allow for backpropagation through the stochastic sampling process, \acp{VAE} utilize the reparametrisation trick. Instead of sampling $z$ directly from $q_\phi(z|x) = \mathcal{N}(\mu, \sigma^2)$, we sample an auxiliary noise variable $\epsilon \sim \mathcal{N}(0, I)$ and compute:

\begin{equation}
    z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon
\end{equation}

where $\odot$ denotes the element-wise product.

A key goal in representation learning is disentanglement, where individual latent units are sensitive to changes in single generative factors of the data while being invariant to others \citep{bengio2013representation}. Formally, a representation is disentangled if it factorizes into independent subspaces corresponding to underlying factors of variation.

Standard \acp{VAE} often fail to learn disentangled representations due to the lack of explicit constraints on the latent structure. To address this, the $\beta$-\ac{VAE} framework \cite{Higgins2016betaVAELB} introduces a hyperparameter $\beta$ to weight the KL divergence term:

\begin{equation}
    \mathcal{L}_{\beta\text{-VAE}} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta D_{KL}(q_\phi(z|x) \| p(z))
\end{equation}

Setting $\beta > 1$ imposes a stronger constraint on the latent bottleneck, encouraging the learned distribution $q_\phi(z|x)$ to align with the isotropic unit Gaussian prior. Since the prior has independent components, this pressure encourages the latent dimensions to become statistically independent, thereby promoting disentanglement, though often at the cost of reconstruction quality.

\subsection{Degrees of Supervision}

While standard supervised learning relies on a fully labelled dataset $\mathcal{D} = \{(x^{(i)}, y^{(i)})\}_{i=1}^N$, obtaining ground truth labels $y$ is often expensive. Semi-supervised learning addresses this by leveraging a small set of labelled data $\mathcal{D}_S$ alongside a much larger set of unlabelled data $\mathcal{D}_U = \{x^{(j)}\}_{j=1}^M$. The objective is to utilize the structural information inherent in $p(x)$ from $\mathcal{D}_U$ to improve the estimation of the conditional distribution $p(y|x)$, typically under the assumption that the decision boundary lies in low-density regions of the input space.

In contrast, weak supervision relaxes the requirement for exact ground truth labels, utilizing instead a dataset $\mathcal{D}_{W} = \{(x^{(i)}, \tilde{y}^{(i)})\}_{i=1}^N$ where $\tilde{y}$ represents a noisy, limited, or heuristic approximation of the true label $y$. A prominent subset is pair learning, where supervision is provided as pairwise constraints $\mathcal{D}_{P} = \{(x^{(i)}, x^{(j)}, r^{(ij)})\}$. Here, $r^{(ij)}$ denotes a binary relationship (e.g., must-link or cannot-link) between samples, guiding the model to learn a representation that respects these relative similarities rather than absolute class assignments.

\subsection{Semi-Supervised Variational Auto-Encoders}

To leverage the generalization benefits of generative modelling for classification tasks, \cite{kingma_semi-supervised_2014} extend the \ac{VAE} framework to the semi-supervised setting. They propose three approaches: the \textit{M1} model, which separates feature learning from downstream classification; the \textit{M2} model, which integrates the class label directly into the latent generative process; and a stacked \textit{M1+M2} architecture. We focus here on the \textit{M2} model, where the data $x$ is generated by both a latent class variable $y$ and a continuous latent variable $z$.

The model describes the data $x$ as being generated by a latent class variable $y$ and a continuous latent variable $z$. The generative process factorizes as:
\begin{equation}
    p_\theta(x, y, z) = p_\theta(x|y, z)p(y)p(z)
\end{equation}
where $p(y) = \text{Cat}(y|\pi)$ and $p(z) = \mathcal{N}(z|0, I)$. In this prior specification, $y$ and $z$ are marginally independent. The likelihood $p_\theta(x|y, z)$ is parameterized by a deep neural network that takes both $y$ and $z$ as inputs to generate the observation $x$.

Crucially, the inference structure introduced to approximate the intractable posterior $p(z, y|x)$ utilizes a specific factorization that introduces a dependency between the latent variables. The recognition model is specified as:
\begin{equation}
    q_\phi(z, y|x) = q_\phi(z|x, y)q_\phi(y|x)
\end{equation}
Here, $q_\phi(y|x)$ is modelled as a categorical distribution, which allows it to be used as a classifier, while $q_\phi(z|x, y)$ is a Gaussian distribution where the parameters (mean and variance) are functions of both the data $x$ and the label $y$. This dependency structure in the inference network allows the model to learn a class-conditional latent distribution for $z$, enabling it to effectively separate style $z$ from class $y$.

In the inference process, however, these variables are coupled. The approximate posterior for the continuous variable is defined as $q_\phi(z|x, y)$. By conditioning on $y$, the network encourages $z$ to capture residual variations orthogonal to class identity.


For the supervised dataset $\mathcal{D}_S = \{(x^m, y^m)\}$, both $x$ and $y$ are observed. The supervised objective $\mathcal{L}_S(\theta, \phi; x, y)$ consists of two main components: a generative term which is the \ac{ELBO} on the joint distribution of $x$, $y$, and $z$ as well as a discriminative term weighted by $\alpha$. By expanding the generative term using the factorization $p_\theta(x,y,z) = p_\theta(x|y,z)p(y)p(z)$, we obtain a reconstruction term which encourages the decoder output to closely match the encoder input, a KL regularization that encourages the style latent $z$ to follow the prior, and the label prior $\log p(y)$:

% \begin{align}
%     \mathcal{L}_S(\theta, \phi; x, y) &= \underbrace{
%         \mathbb{E}_{q_\phi(z|x,y)}\!\left[ \log \frac{p_\theta(x,y,z)}{q_\phi(z|x,y)} \right]
%     }_{\text{Generative (ELBO on joint } x,y)} + \underbrace{
%         \alpha \log q_\phi(y|x)
%     }_{\text{Discriminative}} \\
%     &=\underbrace{\mathbb{E}_{q_\phi(z|x, y)}[\log p_\theta(x|y, z)]}_{\text{Reconstruction}} - \underbrace{D_{KL}(q_\phi(z|x, y) \| p(z))}_{\text{Style Regularisation}} + \underbrace{\log p(y)}_{\text{Class Prior}} + \underbrace{
%         \alpha \log q_\phi(y|x)
%     }_{\text{Discriminative}}
% \end{align}


\begin{align}
    \mathcal{L}_S(\theta, \phi; x, y) &= \underbrace{
        \mathbb{E}_{q_\phi(z|x,y)}\!\left[ \log \frac{p_\theta(x,y,z)}{q_\phi(z|x,y)} \right]
    }_{\text{Generative (ELBO on joint } x,y)} + \underbrace{\alpha \log q_\phi(y|x)}_{\text{Discriminative}}\label{eq:sup_obj}\\
    &= \mathbb{E}_{q_\phi(z|x,y)}\!\bigg[ \log p_\theta(x|y,z) + \log p(y) \notag \\
    &\qquad\qquad + \log p(z) - \log q_\phi(z|x,y) \bigg] + \alpha \log q_\phi(y|x)\\
    &= \underbrace{\mathbb{E}_{q_\phi(z|x, y)}[\log p_\theta(x|y, z)]}_{\text{Reconstruction}} - \underbrace{D_{KL}(q_\phi(z|x, y) \| p(z))}_{\text{Style Regularisation}} \notag \\
    &\qquad\qquad + \underbrace{\log p(y)}_{\text{Label Prior}} + \underbrace{\alpha \log q_\phi(y|x)}_{\text{Discriminative}}
\end{align}


For the unsupervised dataset $\mathcal{D}_U = \{x^n\}$, the label $y$ is treated as a latent variable. We marginalize over all classes to derive the unsupervised bound $\mathcal{L}_U(\theta, \phi; x)$:
\begin{equation}
    \mathcal{L}_U(\theta, \phi; x) = \sum_{y} q_\phi(y|x) \left( \mathbb{E}_{q_\phi(z|x,y)}\!\left[ \log \frac{p_\theta(x,y,z)}{q_\phi(z|x,y)} \right] \right) + \mathcal{H}(q_\phi(y|x))
    \label{eq:unsup_obj}
\end{equation}
where $q_\phi(y|x)$ is the recognition network acting as a discriminative classifier.

The final training objective combines these bounds over both datasets:
\begin{equation}
    \mathcal{L}(\theta,\phi;\mathcal{D}_U,\mathcal{D}_S) = \sum_{x \in \mathcal{D}_U} \mathcal{L}_U(\theta, \phi; x) + \gamma \sum_{(x, y) \in \mathcal{D}_S} \mathcal{L}_S(\theta, \phi; x, y)
    \label{eq:combined_obj}
\end{equation}

Where $\gamma$ is used to balance the contribution of the supervised data. Note that this additional weighting parameter was not part of the original formulation from \cite{kingma_semi-supervised_2014} which solely relied on $\alpha$.

\subsection{Importance Sampling}
\label{sec:imp_sampling}
% Introduce importance sampling

Importance sampling is a Monte Carlo method used to estimate properties of a target distribution $p(x)$ that is difficult to sample from directly, by utilizing a simpler proposal distribution $q(x)$.

Given a target density $p(x)$ and a function $f(x)$, we wish to estimate the expectation $\mathbb{E}_{p}[f(x)]$. We introduce a proposal distribution $q(x)$ such that $q(x) > 0$ whenever $p(x)f(x) \neq 0$ (i.e., the support of $q$ covers the support of $pf$). By multiplying and dividing by $q(x)$, we rewrite the expectation as:
\begin{equation}
    \mathbb{E}_{x \sim p}[f(x)] = \int f(x) p(x) \, dx = \int f(x) \frac{p(x)}{q(x)} q(x) \, dx = \mathbb{E}_{x \sim q}\left[ f(x) \frac{p(x)}{q(x)} \right]
\end{equation}
The term $w(x) = \frac{p(x)}{q(x)}$ is known as the importance weight. In practice, we draw $K$ samples $\{x^{(k)}\}_{k=1}^K$ from the proposal distribution $q(x)$ and approximate the expectation using the sample mean:
\begin{equation}
    \mathbb{E}_{p}[f(x)] \approx \frac{1}{K} \sum_{k=1}^K f(x^{(k)}) w(x^{(k)})
\end{equation}
This standard estimator is unbiased, provided $p(x)$ is a normalized density. 

However, in many applications, the target distribution is only known up to an intractable normalizing constant $Z$, i.e., $p(x) = \frac{\tilde{p}(x)}{Z}$. In this scenario, self-normalized importance sampling is used. We calculate unnormalized weights $\tilde{w}(x) = \frac{\tilde{p}(x)}{q(x)}$ and approximate the expectation using the ratio of estimators:
\begin{equation}
    \mathbb{E}_{p}[f(x)] \approx \frac{\sum_{k=1}^K \tilde{w}(x^{(k)}) f(x^{(k)})}{\sum_{k=1}^K \tilde{w}(x^{(k)})}
\end{equation}
Unlike the standard estimator, the self-normalized estimator is biased for finite $K$ (with bias of order $1/K$), but consistent, converging to the true expectation as $K \to \infty$.

% In the context of VAEs, importance sampling is often used to obtain tighter lower bounds on the log-likelihood (e.g., IWAE \cite{burda2015importance}) or to approximate expectations over complex posteriors where the normalization constant is unknown.