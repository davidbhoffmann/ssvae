% In this work, we review the \acp{SSVAE} by \cite{narayanaswamy_learning_2017}, a unified framework for learning disentangled representations by integrating deep generative models with partially-specified probabilistic graphical models. Unlike standard variational autoencoders that often assume a flat latent space, their approach allows for the injection of domain knowledge through structured dependencies while retaining the flexibility of neural networks for function approximation. By allowing for dependency structures they extend previous work on semi-supervised variational auto-encoders from \cite{kingma_semi-supervised_2014}.

% % What does the literature think of ssvaes
% While most literature builds on the original \ac{SSVAE} paper from \cite{kingma_semi-supervised_2014} (with XX citation son google scholar add reference), there is a also a considerably body of derivative work for \cite{narayanaswamy_learning_2017} (with XX citations on google scholar). However, we only found few works that deeply discuss and critique the method specifically. While \cite{locatello_challenging_2018} validate the use of an inductive bias for disentanglement they also critique the use of labels. This is generally the most frequent critique in the literature \citep{lin_infogan_2020, bouchacourt_multilevel_2017, ke_apgvae_2023, Kim2018DisentanglingBF}. Other papers critique the effect of supervision on disentanglement and mroe general aspects which are not specific to \acp{SSVAE}. We do however find that none of the critiques are substantiate with empirical evidence specific to \acp{SSVAE}.

% % What do my experiments think of ssvaes
% In our experiments, we investigate the critique of labels being potentially noisy, harming the disentanglement process. While we find that noisy labels decrease classification accuracy of the semi-supervised $y$ latent, we also find that a small share of corrupt labels (up to 20\%)only marginally affects performance.  
% We further investigate the effect varying the supervision weight $\alpha$ on the \ac{SSVAE} and find that it improves classification of the $y$ latent whithout leading to overfitting even for values up to 100 and do not affect the final test \ac{ELBO}. 
% For both experiments we find that altering the strenght of supervision, through $\alpha$ or weakening the supervision signal through label corruption, has an effect on the accuracy and the disentanglement of $z$. The unsupervised latent $z$ becomes more disentangled as supervision strength decreases. A potential explanation for this is the semantic conflation problem explained in \autoref{sec:semantic_conflation}. As the model is foreced to stonger prioritise $y$, the $z$ variables are starved of their ability to freely capture rich semantic information. 

% The experiments demonstrate the robustness of \acp{SSVAE} to noisy labels and their sensitivity to the supervision weight $\alpha$. These findings are limited to the \ac{MNIST} dataset and should be verified on other datasets in future research. Another limitation of this work is the limited explored range for the supervision weight parameter and the lack of reporting training accuracy. This makes it hard to assess wether larger alpha values lead to a gap between training and test scores (overfitting). Further research could also investigate joint effects of varying $\alpha$, $\gamma$ and label corruption.


In this work, we reviewed the \ac{SSVAE} framework by \cite{narayanaswamy_learning_2017}, which unifies deep generative models with partially-specified probabilistic graphical models. Unlike standard variational autoencoders that typically assume a flat latent space, this approach allows for the injection of domain knowledge through structured dependencies while retaining the flexibility of neural networks for function approximation. By incorporating arbitrary dependency structures, they extend previous work on semi-supervised variational autoencoders by \cite{kingma_semi-supervised_2014}.


While the majority of the literature builds upon the original \ac{SSVAE} formulation by \cite{kingma_semi-supervised_2014}, there is a considerable body of work deriving from \cite{narayanaswamy_learning_2017}. However, few works critically evaluate the specific limitations of this method. \cite{locatello_challenging_2018} validate the necessity of inductive biases for disentanglement but critique the reliance on labels. This reliance is the most frequent critique in the broader literature \citep{lin_infogan_2020, bouchacourt_multilevel_2017, ke_apgvae_2023, Kim2018DisentanglingBF}. Other works question the effect of supervision on disentanglement generally, rather than specifically addressing the \ac{SSVAE} architecture. Notably, we find that these critiques often lack specific empirical evidence substantiating the failure modes of \acp{SSVAE} under realistic conditions.

Our experiments addressed this gap by investigating the robustness of \acp{SSVAE} to label noise and supervision intensity. 
We investigated the critique that noisy labels might harm the disentanglement process. While we observed that noisy labels decrease the classification accuracy of the semi-supervised latent $y$, the model demonstrated surprising robustness: a corruption rate of up to 20\% only marginally affected performance.

We further analyzed the effect of varying the supervision weight $\alpha$. Increasing $\alpha$ improved the classification of $y$ without leading to overfitting, even for values up to 100, and did not negatively impact the final test \ac{ELBO}. However, we observed a trade-off: altering the strength of supervision, either directly through $\alpha$ and supervised set size or indirectly through label corruption, impacted the disentanglement of the unsupervised latent $z$. specifically, $z$ became more disentangled as supervision strength decreased. A potential explanation for this is the semantic conflation problem discussed in \autoref{sec:semantic_conflation}. We hypothesize that as the model is forced to prioritize the $y$ objective (due to high $\alpha$), the optimization landscape becomes dominated by the classification loss, potentially starving the $z$ variables of the gradient signal required to capture rich, independent semantic information.

These findings demonstrate the robustness of \acp{SSVAE} to noisy labels but also highlight their sensitivity to hyperparameter tuning. These conclusions are currently limited to the \ac{MNIST} dataset; future research should verify these effects on more complex data. Additionally, this study explored a limited range for $\alpha$ and did not extensively track training accuracy, making it difficult to definitively rule out overfitting in all regimes. Future work could investigate the joint effects of varying $\alpha$, the supervision ratio $\gamma$, and label corruption simultaneously.