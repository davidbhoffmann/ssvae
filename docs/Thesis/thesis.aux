\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\AC@reset@newl@bel
\newacro{CCVAE}[CCVAE]{Characteristic Capturing VAE}
\newacro{DLVM}[DLVM]{Deep Latent Variable Model}
\newacro{ELBO}[ELBO]{evidence lower bound}
\newacro{GPPVAE}[GPPVAE]{Gaussian Process Prior VAE}
\newacro{GAN}[GAN]{Generative Adversarial Network}
\newacro{IWAE}[IWAE]{Importance Weighted Autoencoder}
\newacro{i.i.d.}[i.i.d.]{independent and identically distributed}
\newacro{M1}[M1]{latent-feature discriminative model}
\newacro{M1+M2}[M1+M2]{stacked generative semi-supervised model}
\newacro{M2}[M2]{generative semi-supervised model}
\newacro{MIG}[MIG]{Mutual Information Gap}
\newacro{MNIST}[MNIST]{Modified National Institute of Standards and Technology}
\newacro{SVHN}[SVHN]{Street View House Numbers}
\newacro{PoE}[PoE]{Product-of-Experts}
\newacro{SHOT-VAE}[SHOT-VAE]{SmootH-ELBO Optimal InTerpolation VAE}
\newacro{SSVAE}[SSVAE]{Semi-Supervised VAE}
\newacro{VAE}[VAE]{Variational Auto-Encoder}
\newacro{ReLU}[ReLU]{Rectified Linear Units}
\citation{narayanaswamy_learning_2017}
\AC@undonewlabel{acro:SSVAE}
\newlabel{acro:SSVAE}{{}{I}{}{section*.1}{}}
\acronymused{SSVAE}
\AC@undonewlabel{acro:VAE}
\newlabel{acro:VAE}{{}{I}{}{section*.2}{}}
\acronymused{VAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\AC@undonewlabel{acro:MNIST}
\newlabel{acro:MNIST}{{}{I}{}{section*.3}{}}
\acronymused{MNIST}
\citation{bengio2013representation}
\citation{kingma_auto-encoding_2013}
\citation{locatello_challenging_2018}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{intro}{{1}{1}{Introduction}{section.1}{}}
\AC@undonewlabel{acro:VAE}
\newlabel{acro:VAE}{{1}{1}{Introduction}{section*.5}{}}
\acronymused{VAE}
\AC@undonewlabel{acro:SSVAE}
\newlabel{acro:SSVAE}{{1}{1}{Introduction}{section*.6}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{VAE}
\acronymused{SSVAE}
\citation{kingma_auto-encoding_2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Formulation of the \ac {SSVAE} framework. The graphical model partitions the latent space into explicitly supervised variables $y$ (e.g., digit class in MNIST) and unsupervised variables $z$ (e.g., style), governed by arbitrary dependency structures.}}{2}{figure.caption.7}\protected@file@percent }
\acronymused{SSVAE}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:formulation}{{1}{2}{Formulation of the \ac {SSVAE} framework. The graphical model partitions the latent space into explicitly supervised variables $y$ (e.g., digit class in MNIST) and unsupervised variables $z$ (e.g., style), governed by arbitrary dependency structures}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Formulation}{2}{section.2}\protected@file@percent }
\newlabel{background}{{2}{2}{Background and Formulation}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Disentanglement in Variational Auto-Encoders}{2}{subsection.2.1}\protected@file@percent }
\acronymused{VAE}
\acronymused{VAE}
\AC@undonewlabel{acro:ELBO}
\newlabel{acro:ELBO}{{2.1}{2}{Disentanglement in Variational Auto-Encoders}{section*.8}{}}
\acronymused{ELBO}
\acronymused{VAE}
\citation{bengio2013representation}
\citation{Higgins2016betaVAELB}
\citation{chen_isolating_2019}
\citation{kingma_semi-supervised_2014}
\acronymused{VAE}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Degrees of Supervision}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Semi-Supervised Variational Auto-Encoders}{3}{subsection.2.3}\protected@file@percent }
\acronymused{VAE}
\acronymused{ELBO}
\newlabel{eq:sup_obj}{{6}{4}{Semi-Supervised Variational Auto-Encoders}{equation.6}{}}
\citation{kingma_semi-supervised_2014}
\citation{burda2015importance}
\newlabel{eq:unsup_obj}{{9}{5}{Semi-Supervised Variational Auto-Encoders}{equation.9}{}}
\newlabel{eq:combined_obj}{{10}{5}{Semi-Supervised Variational Auto-Encoders}{equation.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Importance Sampling}{5}{subsection.2.4}\protected@file@percent }
\newlabel{sec:imp_sampling}{{2.4}{5}{Importance Sampling}{subsection.2.4}{}}
\acronymused{VAE}
\AC@undonewlabel{acro:IWAE}
\newlabel{acro:IWAE}{{2.4}{5}{Importance Sampling}{section*.9}{}}
\acronymused{IWAE}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{yang_usupervised_2019,biswal_eva_2020}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {3}Semi-Supervised Disentanglement with Arbitrary Dependencies}{7}{section.3}\protected@file@percent }
\newlabel{method}{{3}{7}{Semi-Supervised Disentanglement with Arbitrary Dependencies}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \ac {SSVAE} framework with reformulated supervised (\autoref {eq:sup_obj_reformulated}) and unsupervised (\autoref {eq:unsup_obj}) loss functions.}}{7}{figure.caption.10}\protected@file@percent }
\acronymused{SSVAE}
\newlabel{fig:framework_obj}{{2}{7}{\ac {SSVAE} framework with reformulated supervised (\autoref {eq:sup_obj_reformulated}) and unsupervised (\autoref {eq:unsup_obj}) loss functions}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Generalised Variational Objective}{7}{subsection.3.1}\protected@file@percent }
\citation{narayanaswamy_learning_2017}
\newlabel{eq:sup_obj_reformulated}{{16}{8}{The Generalised Variational Objective}{equation.16}{}}
\newlabel{eq:expectation_est}{{18}{8}{The Generalised Variational Objective}{equation.18}{}}
\newlabel{eq:disc_est}{{19}{8}{The Generalised Variational Objective}{equation.19}{}}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{kingma2017adammethodstochasticoptimization}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{deng2012mnist}
\citation{netzer2011reading}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Relation to Kingma's M2 Model}{9}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Graphical Model and Stochastic Implementation}{9}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Experiments and Findings}{9}{subsection.3.4}\protected@file@percent }
\newlabel{sec:their_experiments}{{3.4}{9}{Experiments and Findings}{subsection.3.4}{}}
\AC@undonewlabel{acro:MNIST}
\newlabel{acro:MNIST}{{3.4}{9}{Experiments and Findings}{section*.11}{}}
\acronymused{MNIST}
\AC@undonewlabel{acro:SVHN}
\newlabel{acro:SVHN}{{3.4}{9}{Experiments and Findings}{section*.12}{}}
\acronymused{SVHN}
\acronymused{MNIST}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{GeBeKr01}
\citation{pmlr-v38-jampani15}
\citation{pmlr-v38-jampani15}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Benchmark Classification: MNIST and SVHN}{10}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{sec:mnist}{{3.4.1}{10}{Benchmark Classification: MNIST and SVHN}{subsubsection.3.4.1}{}}
\acronymused{MNIST}
\acronymused{SVHN}
\acronymused{MNIST}
\acronymused{SVHN}
\acronymused{SSVAE}
\acronymused{MNIST}
\acronymused{SVHN}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Intrinsic Faces: Disentangling Continuous Factors}{10}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{sec:exp_faces}{{3.4.2}{10}{Intrinsic Faces: Disentangling Continuous Factors}{subsubsection.3.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Multi-MNIST: Stochastic Dimensionality and Compositionality}{11}{subsubsection.3.4.3}\protected@file@percent }
\acronymused{MNIST}
\acronymused{MNIST}
\acronymused{MNIST}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{GeBeKr01}
\citation{pmlr-v38-jampani15}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {4}Extensions and Limitations}{12}{section.4}\protected@file@percent }
\newlabel{extensions}{{4}{12}{Extensions and Limitations}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Own Criticism}{12}{subsection.4.1}\protected@file@percent }
\newlabel{sec:my_critique}{{4.1}{12}{Own Criticism}{subsection.4.1}{}}
\newlabel{eq:yaleb}{{23}{12}{Own Criticism}{equation.23}{}}
\acronymused{MNIST}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{pmlr-v38-jampani15}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{casale_gaussian_2018}
\citation{narayanaswamy_learning_2017}
\citation{mattei_leveraging_2018}
\citation{locatello_challenging_2018}
\acronymused{MNIST}
\acronymused{SVHN}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}General Critiques}{13}{subsection.4.2}\protected@file@percent }
\newlabel{sec:general_critique}{{4.2}{13}{General Critiques}{subsection.4.2}{}}
\AC@undonewlabel{acro:i.i.d.}
\newlabel{acro:i.i.d.}{{4.2}{13}{General Critiques}{section*.13}{}}
\acronymused{i.i.d.}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Impossibility of Unsupervised Disentanglement}{13}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{sec:impossibility_theorem}{{4.2.1}{13}{Impossibility of Unsupervised Disentanglement}{subsubsection.4.2.1}{}}
\acronymused{VAE}
\acronymused{ELBO}
\citation{locatello_challenging_2018}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{locatello_challenging_2018}
\citation{narayanaswamy_learning_2017}
\citation{casale_gaussian_2018}
\citation{narayanaswamy_learning_2017}
\citation{casale_gaussian_2018}
\citation{casale_gaussian_2018}
\citation{casale_gaussian_2018}
\citation{narayanaswamy_learning_2017}
\citation{esmaeili_structured_2018}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}The Limitation of Isotropic Priors}{14}{subsubsection.4.2.2}\protected@file@percent }
\acronymused{i.i.d.}
\AC@undonewlabel{acro:GPPVAE}
\newlabel{acro:GPPVAE}{{4.2.2}{14}{The Limitation of Isotropic Priors}{section*.14}{}}
\acronymused{GPPVAE}
\citation{mattei_leveraging_2018}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{mattei_leveraging_2018}
\citation{narayanaswamy_learning_2017}
\citation{joy_capturing_2020}
\citation{joy_capturing_2020}
\acronymused{ELBO}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Unbounded Likelihoods and Mode Collapse}{15}{subsubsection.4.2.3}\protected@file@percent }
\AC@undonewlabel{acro:DLVM}
\newlabel{acro:DLVM}{{4.2.3}{15}{Unbounded Likelihoods and Mode Collapse}{section*.15}{}}
\acronymused{DLVM}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Disentanglement Implications and Limitations}{15}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Semantic Conflation Problem}{15}{subsubsection.4.3.1}\protected@file@percent }
\newlabel{sec:semantic_conflation}{{4.3.1}{15}{Semantic Conflation Problem}{subsubsection.4.3.1}{}}
\citation{joy_capturing_2020}
\citation{narayanaswamy_learning_2017}
\citation{joy_capturing_2020}
\citation{nie_semi_supervised_2020}
\citation{adel_discovering_2018}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\citation{narayanaswamy_learning_2017}
\citation{feng_shot_vae_2021}
\citation{narayanaswamy_learning_2017}
\acronymused{VAE}
\AC@undonewlabel{acro:CCVAE}
\newlabel{acro:CCVAE}{{4.3.1}{16}{Semantic Conflation Problem}{section*.16}{}}
\acronymused{CCVAE}
\acronymused{SSVAE}
\acronymused{CCVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Breaking the ELBO Bottleneck}{16}{subsubsection.4.3.2}\protected@file@percent }
\newlabel{sec:elbo_bottleneck}{{4.3.2}{16}{Breaking the ELBO Bottleneck}{subsubsection.4.3.2}{}}
\acronymused{SSVAE}
\acronymused{ELBO}
\AC@undonewlabel{acro:SHOT-VAE}
\newlabel{acro:SHOT-VAE}{{4.3.2}{16}{Breaking the ELBO Bottleneck}{section*.17}{}}
\acronymused{SHOT-VAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{ELBO}
\citation{feng_shot_vae_2021}
\citation{kingma_semi-supervised_2014}
\citation{shu_weakly_2019}
\citation{shu_weakly_2019}
\citation{narayanaswamy_learning_2017}
\citation{chen_isolating_2019}
\citation{locatello_challenging_2018}
\citation{shu_weakly_2019}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{locatello_weakly_2020}
\citation{joy_learning_2021}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{SHOT-VAE}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{SHOT-VAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Differentiating Consistency and Restrictiveness}{17}{subsubsection.4.3.3}\protected@file@percent }
\AC@undonewlabel{acro:MIG}
\newlabel{acro:MIG}{{4.3.3}{17}{Differentiating Consistency and Restrictiveness}{section*.18}{}}
\acronymused{MIG}
\citation{locatello_challenging_2018}
\citation{lin_infogan_2020,bouchacourt_multilevel_2017,ke_apgvae_2023,Kim2018DisentanglingBF}
\citation{narayanaswamy_learning_2017}
\citation{Kim2018DisentanglingBF}
\citation{narayanaswamy_learning_2017}
\citation{perry2010continuous}
\citation{locatello_weakly_2020}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{chen_weakly_2020}
\citation{narayanaswamy_learning_2017}
\citation{locatello_weakly_2020}
\citation{locatello_weakly_2020}
\citation{chen_weakly_2020}
\citation{yang_disentangling_2019}
\citation{locatello_challenging_2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Extensions in Supervision: From Semi to Weak}{18}{subsection.4.4}\protected@file@percent }
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}The Weak Supervision Paradigm}{18}{subsubsection.4.4.1}\protected@file@percent }
\newlabel{sec:ext_weak_paradigm}{{4.4.1}{18}{The Weak Supervision Paradigm}{subsubsection.4.4.1}{}}
\citation{joy_learning_2021}
\citation{narayanaswamy_learning_2017}
\citation{joy_learning_2021}
\citation{locatello_weakly_2020}
\citation{narayanaswamy_learning_2017}
\citation{gordon_meta_2019,kulinski_explaining_2023}
\citation{narayanaswamy_learning_2017}
\citation{vaze_representation_2023}
\citation{narayanaswamy_learning_2017}
\citation{yang_usupervised_2019}
\citation{narayanaswamy_learning_2017}
\citation{yang_usupervised_2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Multimodal VAEs and Mutual Supervision}{19}{subsubsection.4.4.2}\protected@file@percent }
\acronymused{VAE}
\AC@undonewlabel{acro:PoE}
\newlabel{acro:PoE}{{4.4.2}{19}{Multimodal VAEs and Mutual Supervision}{section*.19}{}}
\acronymused{PoE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Applications and Integrations in the Literature}{19}{subsection.4.5}\protected@file@percent }
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Domain Adaptation in the Medical Domain}{19}{subsubsection.4.5.1}\protected@file@percent }
\acronymused{SSVAE}
\citation{narayanaswamy_learning_2017}
\citation{biswal_eva_2020}
\citation{li_multi_2018}
\citation{zhang_causal_2020}
\citation{narayanaswamy_learning_2017}
\citation{ye_learning_2020}
\citation{narayanaswamy_learning_2017}
\citation{ye_learning_2020}
\citation{kingma_semi-supervised_2014}
\acronymused{SSVAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Causal Extension and Robustness}{20}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Catastrophic Forgetting and Generative Replay}{20}{subsubsection.4.5.3}\protected@file@percent }
\acronymused{VAE}
\AC@undonewlabel{acro:GAN}
\newlabel{acro:GAN}{{4.5.3}{20}{Catastrophic Forgetting and Generative Replay}{section*.20}{}}
\acronymused{GAN}
\acronymused{SSVAE}
\citation{ye_lifelong_2022}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\AC@undonewlabel{acro:M1+M2}
\newlabel{acro:M1+M2}{{4.5.3}{21}{Catastrophic Forgetting and Generative Replay}{section*.21}{}}
\acronymused{M1+M2}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\citation{deng2012mnist}
\citation{narayanaswamy_learning_2017}
\citation{nair2010rectified}
\citation{nair2010rectified}
\citation{narayanaswamy_learning_2017}
\citation{kingma2017adammethodstochasticoptimization}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{Higgins2016betaVAELB}
\citation{Kim2018DisentanglingBF}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{22}{section.5}\protected@file@percent }
\newlabel{experiments}{{5}{22}{Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Implementation Details}{22}{subsection.5.1}\protected@file@percent }
\newlabel{sec:implementation_details}{{5.1}{22}{Implementation Details}{subsection.5.1}{}}
\acronymused{MNIST}
\acronymused{SSVAE}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \ac {SSVAE} architecture used throughout all our experiments. The encoder has $28\times 28=784$ inputs $\{x_i\}_{i=1}^{784}$ matching the \ac {MNIST} input and the decoder has the same number of output neurons with sigmoid activations and a binary cross-entropy loss. Both encoder and decoder have 256 hidden neurons $\{h_i\}_{i=1}^{256}$ with \ac {ReLU} \citep  {nair2010rectified} activations. The latent space has 10 style variables $\{z_i\}_{i=1}^{10}$ and 10 digit neurons $\{y_i\}_{i=1}^{10}$. The decoder uses sampled $\{z'_i\}_{i=1}^{10}$ from a Gaussian and $\{y'_i\}_{i=1}^{10}$ from a Gumbel-Softmax distribution as inputs. Note that the dots between the neurons of a layer symbolise the remaining not shown neurons. Dashed lines symbolise the reparametrization trick and sampling process.}}{22}{figure.caption.22}\protected@file@percent }
\acronymused{SSVAE}
\acronymused{MNIST}
\AC@undonewlabel{acro:ReLU}
\newlabel{acro:ReLU}{{3}{22}{\ac {SSVAE} architecture used throughout all our experiments. The encoder has $28\times 28=784$ inputs $\{x_i\}_{i=1}^{784}$ matching the \ac {MNIST} input and the decoder has the same number of output neurons with sigmoid activations and a binary cross-entropy loss. Both encoder and decoder have 256 hidden neurons $\{h_i\}_{i=1}^{256}$ with \ac {ReLU} \citep {nair2010rectified} activations. The latent space has 10 style variables $\{z_i\}_{i=1}^{10}$ and 10 digit neurons $\{y_i\}_{i=1}^{10}$. The decoder uses sampled $\{z'_i\}_{i=1}^{10}$ from a Gaussian and $\{y'_i\}_{i=1}^{10}$ from a Gumbel-Softmax distribution as inputs. Note that the dots between the neurons of a layer symbolise the remaining not shown neurons. Dashed lines symbolise the reparametrization trick and sampling process}{section*.23}{}}
\acronymused{ReLU}
\newlabel{fig:architecture}{{3}{22}{\ac {SSVAE} architecture used throughout all our experiments. The encoder has $28\times 28=784$ inputs $\{x_i\}_{i=1}^{784}$ matching the \ac {MNIST} input and the decoder has the same number of output neurons with sigmoid activations and a binary cross-entropy loss. Both encoder and decoder have 256 hidden neurons $\{h_i\}_{i=1}^{256}$ with \ac {ReLU} \citep {nair2010rectified} activations. The latent space has 10 style variables $\{z_i\}_{i=1}^{10}$ and 10 digit neurons $\{y_i\}_{i=1}^{10}$. The decoder uses sampled $\{z'_i\}_{i=1}^{10}$ from a Gaussian and $\{y'_i\}_{i=1}^{10}$ from a Gumbel-Softmax distribution as inputs. Note that the dots between the neurons of a layer symbolise the remaining not shown neurons. Dashed lines symbolise the reparametrization trick and sampling process}{figure.caption.22}{}}
\acronymused{MNIST}
\citation{narayanaswamy_learning_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Supervision Weight} experiment results with varying $\alpha $ for supervised set sizes $M$ of 100 ($0.001\bar  {6}\%$), 600 ($0.01\%$), 1000 ($0.01\bar  {6}\%$) and 3000 ($0.05\%$). Error bars indicate variance across 10 random seeds. Note that the y-scales vary and do not start at zero. \textbf  {Top Left:} final test \ac {ELBO} for varying $\alpha $ remains relatively stable. \textbf  {Bottom Left:} final test accuracy of $y$ improves for higher values of $\alpha $. \textbf  {Right:} disentanglement metrics decrease with increasing $\alpha $.}}{23}{figure.caption.24}\protected@file@percent }
\acronymused{ELBO}
\newlabel{fig:alpha_exp}{{4}{23}{\textbf {Supervision Weight} experiment results with varying $\alpha $ for supervised set sizes $M$ of 100 ($0.001\bar {6}\%$), 600 ($0.01\%$), 1000 ($0.01\bar {6}\%$) and 3000 ($0.05\%$). Error bars indicate variance across 10 random seeds. Note that the y-scales vary and do not start at zero. \textbf {Top Left:} final test \ac {ELBO} for varying $\alpha $ remains relatively stable. \textbf {Bottom Left:} final test accuracy of $y$ improves for higher values of $\alpha $. \textbf {Right:} disentanglement metrics decrease with increasing $\alpha $}{figure.caption.24}{}}
\acronymused{MIG}
\acronymused{ELBO}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Supervision Weight}{23}{subsection.5.2}\protected@file@percent }
\newlabel{sec:alpha_exp}{{5.2}{23}{Supervision Weight}{subsection.5.2}{}}
\acronymused{MNIST}
\acronymused{MNIST}
\acronymused{SSVAE}
\citation{lin_infogan_2020,bouchacourt_multilevel_2017,ke_apgvae_2023,Kim2018DisentanglingBF}
\citation{narayanaswamy_learning_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Label Corruption} experiment results with varying label corruption rate for supervised set sizes $M$ of 100 ($0.001\bar  {6}\%$), 600 ($0.01\%$), 1000 ($0.01\bar  {6}\%$) and 3000 ($0.05\%$). Error bars indicate variance across 10 random seeds. Note that the y-scales vary and do not start at zero. \textbf  {Top Left:} final test \ac {ELBO} for varying noise levels remains relatively stable. \textbf  {Bottom Left:} while the final test accuracy of $y$ decreases for higher corruption rates, it remains relatively high for up to 20\% noise. \textbf  {Right:} disentanglement increases with a higher corruption rate across all three metrics.}}{24}{figure.caption.25}\protected@file@percent }
\acronymused{ELBO}
\newlabel{fig:noise_exp}{{5}{24}{\textbf {Label Corruption} experiment results with varying label corruption rate for supervised set sizes $M$ of 100 ($0.001\bar {6}\%$), 600 ($0.01\%$), 1000 ($0.01\bar {6}\%$) and 3000 ($0.05\%$). Error bars indicate variance across 10 random seeds. Note that the y-scales vary and do not start at zero. \textbf {Top Left:} final test \ac {ELBO} for varying noise levels remains relatively stable. \textbf {Bottom Left:} while the final test accuracy of $y$ decreases for higher corruption rates, it remains relatively high for up to 20\% noise. \textbf {Right:} disentanglement increases with a higher corruption rate across all three metrics}{figure.caption.25}{}}
\acronymused{ELBO}
\acronymused{ELBO}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Label Corruption}{24}{subsection.5.3}\protected@file@percent }
\newlabel{sec:noise_exp}{{5.3}{24}{Label Corruption}{subsection.5.3}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{MNIST}
\acronymused{ELBO}
\acronymused{ELBO}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{lin_infogan_2020,bouchacourt_multilevel_2017,ke_apgvae_2023,Kim2018DisentanglingBF}
\citation{feng_shot_vae_2021}
\citation{joy_capturing_2020}
\citation{feng_shot_vae_2021}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{26}{section.6}\protected@file@percent }
\newlabel{discussion}{{6}{26}{Discussion}{section.6}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{ELBO}
\acronymused{SSVAE}
\acronymused{MNIST}
\acronymused{ELBO}
\citation{narayanaswamy_learning_2017}
\citation{locatello_challenging_2018}
\citation{joy_learning_2021}
\citation{locatello_weakly_2020}
\citation{zhang_causal_2020}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{28}{section.7}\protected@file@percent }
\newlabel{conclusion}{{7}{28}{Conclusion}{section.7}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\acronymused{SSVAE}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\citation{kingma_semi-supervised_2014}
\citation{narayanaswamy_learning_2017}
\@writefile{toc}{\contentsline {section}{\numberline {A}Systematic Review Process}{IV}{appendix.A}\protected@file@percent }
\newlabel{apx:systematic_review}{{A}{IV}{Systematic Review Process}{appendix.A}{}}
\acronymused{SSVAE}
\acronymused{SSVAE}
\@writefile{toc}{\contentsline {section}{\numberline {B}SSVAE Training}{IV}{appendix.B}\protected@file@percent }
\newlabel{apx:training}{{B}{IV}{SSVAE Training}{appendix.B}{}}
\newlabel{fig:train_alpha}{{6a}{IV}{All training runs of the \textbf {supervision weight experiment} in \autoref {sec:alpha_exp}}{figure.caption.26}{}}
\newlabel{sub@fig:train_alpha}{{a}{IV}{All training runs of the \textbf {supervision weight experiment} in \autoref {sec:alpha_exp}}{figure.caption.26}{}}
\newlabel{fig:train_noise}{{6b}{IV}{All training runs of the \textbf {corruption rate experiment} in \autoref {sec:noise_exp}}{figure.caption.26}{}}
\newlabel{sub@fig:train_noise}{{b}{IV}{All training runs of the \textbf {corruption rate experiment} in \autoref {sec:noise_exp}}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Supervision Weight Results}{V}{appendix.C}\protected@file@percent }
\newlabel{sec:results_alpha_exp}{{C}{V}{Supervision Weight Results}{appendix.C}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Aggregated results for the supervision weight experiment.}}{V}{table.caption.27}\protected@file@percent }
\newlabel{tab:alpha_exp}{{1}{V}{Aggregated results for the supervision weight experiment}{table.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Detailed results for the supervision weight experiment.}}{V}{table.2}\protected@file@percent }
\newlabel{tab:full_alpha_exp}{{2}{V}{Detailed results for the supervision weight experiment}{table.2}{}}
\gdef \LT@i {\LT@entry 
    {6}{39.77788pt}\LT@entry 
    {5}{32.00008pt}\LT@entry 
    {1}{32.00005pt}\LT@entry 
    {2}{42.21184pt}\LT@entry 
    {2}{63.97232pt}\LT@entry 
    {1}{32.62505pt}\LT@entry 
    {2}{61.91676pt}\LT@entry 
    {2}{74.58342pt}\LT@entry 
    {1}{0.0pt}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Corruption Rate Results}{XIII}{appendix.D}\protected@file@percent }
\newlabel{sec:results_corruption_exp}{{D}{XIII}{Corruption Rate Results}{appendix.D}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Aggregated results for the corruption rate experiment.}}{XIII}{table.caption.28}\protected@file@percent }
\newlabel{tab:corruption_exp}{{3}{XIII}{Aggregated results for the corruption rate experiment}{table.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Detailed results for the corruption rate experiment.}}{XIII}{table.4}\protected@file@percent }
\newlabel{tab:full_corruption_exp}{{4}{XIII}{Detailed results for the corruption rate experiment}{table.4}{}}
\gdef \LT@ii {\LT@entry 
    {1}{47.3334pt}\LT@entry 
    {5}{32.00008pt}\LT@entry 
    {1}{32.00005pt}\LT@entry 
    {1}{42.21184pt}\LT@entry 
    {1}{63.97232pt}\LT@entry 
    {2}{32.62505pt}\LT@entry 
    {1}{61.91676pt}\LT@entry 
    {1}{74.58342pt}\LT@entry 
    {1}{0.0pt}}
\bibdata{bibliography}
\bibcite{adel_discovering_2018}{{1}{2018}{{Adel et~al.}}{{Adel, Ghahramani and\ Weller}}}
\bibcite{bengio2013representation}{{2}{2013}{{Bengio et~al.}}{{Bengio, Courville and\ Vincent}}}
\bibcite{biswal_eva_2020}{{3}{2021}{{Biswal et~al.}}{{Biswal, Ghosh, Duke, Malin, Stewart, Xiao and\ Sun}}}
\bibcite{bouchacourt_multilevel_2017}{{4}{2018}{{Bouchacourt et~al.}}{{Bouchacourt, Tomioka and\ Nowozin}}}
\bibcite{burda2015importance}{{5}{2016}{{Burda et~al.}}{{Burda, Grosse and\ Salakhutdinov}}}
\bibcite{casale_gaussian_2018}{{6}{2018}{{Casale et~al.}}{{Casale, Dalca, Saglietti, Listgarten and\ Fusi}}}
\bibcite{chen_weakly_2020}{{7}{2020}{{Chen and\ Batmanghelich}}{{}}}
\bibcite{chen_isolating_2019}{{8}{2018}{{Chen et~al.}}{{Chen, Li, Grosse and\ Duvenaud}}}
\bibcite{deng2012mnist}{{9}{2012}{{Deng}}{{}}}
\bibcite{esmaeili_structured_2018}{{10}{2019}{{Esmaeili et~al.}}{{Esmaeili, Wu, Jain, Bozkurt, Siddharth, Paige, Brooks, Dy and\ Meent}}}
\bibcite{feng_shot_vae_2021}{{11}{2021}{{Feng et~al.}}{{Feng, Kong, Chen, Zhang, Zhu and\ Chen}}}
\bibcite{GeBeKr01}{{12}{2001}{{Georghiades et~al.}}{{Georghiades, Belhumeur and\ Kriegman}}}
\bibcite{gordon_meta_2019}{{13}{2019}{{Gordon et~al.}}{{Gordon, Bronskill, Bauer, Nowozin and\ Turner}}}
\bibcite{Higgins2016betaVAELB}{{14}{2017}{{Higgins et~al.}}{{Higgins, Matthey, Pal, Burgess, Glorot, Botvinick, Mohamed and\ Lerchner}}}
\bibcite{pmlr-v38-jampani15}{{15}{2015}{{Jampani et~al.}}{{Jampani, Eslami, Tarlow, Kohli and\ Winn}}}
\bibcite{joy_capturing_2020}{{16}{2021}{{Joy et~al.}}{{Joy, Schmon, Torr, Siddharth and\ Rainforth}}}
\bibcite{joy_learning_2021}{{17}{2022}{{Joy et~al.}}{{Joy, Shi, Torr, Rainforth, Schmon and\ Siddharth}}}
\bibcite{ke_apgvae_2023}{{18}{2024}{{Ke et~al.}}{{Ke, Jing, Wo\'{z}niak, Xu, Liang and\ Zheng}}}
\bibcite{Kim2018DisentanglingBF}{{19}{2018}{{Kim and\ Mnih}}{{}}}
\bibcite{kingma2017adammethodstochasticoptimization}{{20}{2015}{{Kingma and\ Ba}}{{}}}
\bibcite{kingma_semi-supervised_2014}{{21}{2014}{{Kingma et~al.}}{{Kingma, Rezende, Mohamed and\ Welling}}}
\bibcite{kingma_auto-encoding_2013}{{22}{2014}{{Kingma and\ Welling}}{{}}}
\bibcite{kulinski_explaining_2023}{{23}{2023}{{Kulinski and\ Inouye}}{{}}}
\bibcite{li_multi_2018}{{24}{2018}{{Li et~al.}}{{Li, Zhang and\ Liu}}}
\bibcite{lin_infogan_2020}{{25}{2020}{{Lin et~al.}}{{Lin, Thekumparampil, Fanti and\ Oh}}}
\bibcite{locatello_challenging_2018}{{26}{2019}{{Locatello et~al.}}{{Locatello, Bauer, Lucic, Gelly, Schoelkopf and\ Bachem}}}
\bibcite{locatello_weakly_2020}{{27}{2020}{{Locatello et~al.}}{{Locatello, Poole, Raetsch, Schoelkopf, Bachem and\ Tschannen}}}
\bibcite{mattei_leveraging_2018}{{28}{2018}{{Mattei and\ Frellsen}}{{}}}
\bibcite{nair2010rectified}{{29}{2010}{{Nair and\ Hinton}}{{}}}
\bibcite{narayanaswamy_learning_2017}{{30}{2017}{{Narayanaswamy et~al.}}{{Narayanaswamy, Paige, van~de Meent, Desmaison, Goodman, Kohli, Wood and\ Torr}}}
\bibcite{netzer2011reading}{{31}{2011}{{Netzer et~al.}}{{Netzer, Wang, Coates, Bissacco, Wu, Ng et~al.}}}
\bibcite{nie_semi_supervised_2020}{{32}{2020}{{Nie et~al.}}{{Nie, Karras, Garg, Debnath, Patney, Patel and\ Anandkumar}}}
\bibcite{perry2010continuous}{{33}{2010}{{Perry et~al.}}{{Perry, Rolls and\ Stringer}}}
\bibcite{shu_weakly_2019}{{34}{2020}{{Shu et~al.}}{{Shu, Chen, Kumar, Ermon and\ Poole}}}
\bibcite{vaze_representation_2023}{{35}{2023}{{Vaze et~al.}}{{Vaze, Vedaldi and\ Zisserman}}}
\bibcite{yang_usupervised_2019}{{36}{2019}{{Yang et~al.}}{{Yang, Dvornek, Zhang, Chapiro, Lin and\ Duncan}}}
\bibcite{yang_disentangling_2019}{{37}{2019}{{Yang and\ Yao}}{{}}}
\bibcite{ye_learning_2020}{{38}{2020}{{Ye and\ Bors}}{{}}}
\bibcite{ye_lifelong_2022}{{39}{2022}{{Ye and\ Bors}}{{}}}
\bibcite{zhang_causal_2020}{{40}{2020}{{Zhang et~al.}}{{Zhang, Zhang and\ Li}}}
\bibstyle{dcu}
\citation{narayanaswamy_learning_2017}
\citation{narayanaswamy_learning_2017}
\gdef \@abspage@last{52}
